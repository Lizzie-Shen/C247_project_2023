{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfbkaBAt4Iqd"
   },
   "source": [
    "### This is the notebook for using different time sequence to evaluate the performance of CNN and CNN_LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptCyH4RI4Iqi"
   },
   "source": [
    "## data preparation and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-17T18:57:27.998745Z",
     "iopub.status.busy": "2023-03-17T18:57:27.998171Z",
     "iopub.status.idle": "2023-03-17T18:57:43.232205Z",
     "shell.execute_reply": "2023-03-17T18:57:43.230643Z",
     "shell.execute_reply.started": "2023-03-17T18:57:27.998713Z"
    },
    "id": "gOOuBFCN4Iqi"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten,Dropout\n",
    "from keras.layers import Conv2D,BatchNormalization,MaxPooling2D,Reshape,LSTM,MaxPooling1D,Conv1D\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "np.random.seed(12345)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "execution": {
     "iopub.execute_input": "2023-03-17T18:59:31.887978Z",
     "iopub.status.busy": "2023-03-17T18:59:31.886858Z",
     "iopub.status.idle": "2023-03-17T18:59:36.845628Z",
     "shell.execute_reply": "2023-03-17T18:59:36.842613Z",
     "shell.execute_reply.started": "2023-03-17T18:59:31.887936Z"
    },
    "executionInfo": {
     "elapsed": 7685,
     "status": "ok",
     "timestamp": 1678915156254,
     "user": {
      "displayName": "CHUSHU SHEN",
      "userId": "01882361236256093541"
     },
     "user_tz": 420
    },
    "id": "SbozQTlO4Iqk",
    "outputId": "c68ec5da-7ddf-4ef6-c243-554268874f14"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f0bc5e51e90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Loading and visualizing the data\n",
    "\n",
    "## Loading the dataset\n",
    "\n",
    "\n",
    "X_test = np.load(\"/kaggle/input/eeg-247-project/X_test.npy\")\n",
    "y_test = np.load(\"/kaggle/input/eeg-247-project/y_test.npy\")\n",
    "person_train_valid = np.load(\"/kaggle/input/eeg-247-project/person_train_valid.npy\")\n",
    "X_train_valid = np.load(\"/kaggle/input/eeg-247-project/X_train_valid.npy\")\n",
    "y_train_valid = np.load(\"/kaggle/input/eeg-247-project/y_train_valid.npy\")\n",
    "person_test = np.load(\"/kaggle/input/eeg-247-project/person_test.npy\")\n",
    "\n",
    "## Adjusting the labels so that \n",
    "\n",
    "# Cue onset left - 0\n",
    "# Cue onset right - 1\n",
    "# Cue onset foot - 2\n",
    "# Cue onset tongue - 3\n",
    "\n",
    "y_train_valid -= 769\n",
    "y_test -= 769\n",
    "\n",
    "y_test_original=y_test#store original test label\n",
    "\n",
    "## Visualizing the data\n",
    "\n",
    "ch_data = X_train_valid[:,8,:] # extracts the 9th channel from the data\n",
    "\n",
    "\n",
    "class_0_ind = np.where(y_train_valid == 0) # finds the indices where the label is 0\n",
    "ch_data_class_0 = ch_data[class_0_ind] # finds the data where label is 0\n",
    "avg_ch_data_class_0 = np.mean(ch_data_class_0,axis=0) # finds the average representation of the 9th channel when label is 0\n",
    "\n",
    "\n",
    "class_1_ind = np.where(y_train_valid == 1)\n",
    "ch_data_class_1 = ch_data[class_1_ind]\n",
    "avg_ch_data_class_1 = np.mean(ch_data_class_1,axis=0)\n",
    "\n",
    "class_2_ind = np.where(y_train_valid == 2)\n",
    "ch_data_class_2 = ch_data[class_2_ind]\n",
    "avg_ch_data_class_2 = np.mean(ch_data_class_2,axis=0)\n",
    "\n",
    "class_3_ind = np.where(y_train_valid == 3)\n",
    "ch_data_class_3 = ch_data[class_3_ind]\n",
    "avg_ch_data_class_3 = np.mean(ch_data_class_3,axis=0)\n",
    "\n",
    "\n",
    "plt.plot(np.arange(1000),avg_ch_data_class_0)\n",
    "plt.plot(np.arange(1000),avg_ch_data_class_1)\n",
    "plt.plot(np.arange(1000),avg_ch_data_class_2)\n",
    "plt.plot(np.arange(1000),avg_ch_data_class_3)\n",
    "plt.axvline(x=500, label='line at t=500',c='cyan')\n",
    "\n",
    "plt.legend([\"Cue Onset left\", \"Cue Onset right\", \"Cue onset foot\", \"Cue onset tongue\"])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-03-17T18:59:42.221271Z",
     "iopub.status.busy": "2023-03-17T18:59:42.220663Z",
     "iopub.status.idle": "2023-03-17T18:59:45.058030Z",
     "shell.execute_reply": "2023-03-17T18:59:45.056829Z",
     "shell.execute_reply.started": "2023-03-17T18:59:42.221234Z"
    },
    "executionInfo": {
     "elapsed": 2773,
     "status": "ok",
     "timestamp": 1678915490920,
     "user": {
      "displayName": "CHUSHU SHEN",
      "userId": "01882361236256093541"
     },
     "user_tz": 420
    },
    "id": "9oHT_g2m4Iql",
    "outputId": "fc9313d0-50f3-45c7-eef1-66fda88f9a55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X after trimming: (2115, 22, 500)\n",
      "Shape of X after maxpooling: (2115, 22, 250)\n",
      "Shape of X after averaging+noise and concatenating: (4230, 22, 250)\n",
      "Shape of X after subsampling and concatenating: (8460, 22, 250)\n"
     ]
    }
   ],
   "source": [
    "def data_prep(X,y,sub_sample,average,noise,trim_time=500):\n",
    "    \n",
    "    total_X = None\n",
    "    total_y = None\n",
    "    \n",
    "    # Trimming the data (sample,22,1000) -> (sample,22,500)\n",
    "    X = X[:,:,0:trim_time]\n",
    "    print('Shape of X after trimming:',X.shape)\n",
    "    \n",
    "    # Maxpooling the data (sample,22,1000) -> (sample,22,500/sub_sample)\n",
    "    X_max = np.max(X.reshape(X.shape[0], X.shape[1], -1, sub_sample), axis=3)\n",
    "    \n",
    "    \n",
    "    total_X = X_max\n",
    "    total_y = y\n",
    "    print('Shape of X after maxpooling:',total_X.shape)\n",
    "    \n",
    "    # Averaging + noise \n",
    "    X_average = np.mean(X.reshape(X.shape[0], X.shape[1], -1, average),axis=3)\n",
    "    X_average = X_average + np.random.normal(0.0, 0.5, X_average.shape)\n",
    "    \n",
    "    total_X = np.vstack((total_X, X_average))\n",
    "    total_y = np.hstack((total_y, y))\n",
    "    print('Shape of X after averaging+noise and concatenating:',total_X.shape)\n",
    "    \n",
    "    # Subsampling\n",
    "    \n",
    "    for i in range(sub_sample):\n",
    "        \n",
    "        X_subsample = X[:, :, i::sub_sample] + \\\n",
    "                            (np.random.normal(0.0, 0.5, X[:, :,i::sub_sample].shape) if noise else 0.0)\n",
    "            \n",
    "        total_X = np.vstack((total_X, X_subsample))\n",
    "        total_y = np.hstack((total_y, y))\n",
    "        \n",
    "    \n",
    "    print('Shape of X after subsampling and concatenating:',total_X.shape)\n",
    "    return total_X,total_y\n",
    "\n",
    "\n",
    "X_train_valid_prep,y_train_valid_prep = data_prep(X_train_valid,y_train_valid,2,2,True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-17T18:59:54.174627Z",
     "iopub.status.busy": "2023-03-17T18:59:54.173879Z",
     "iopub.status.idle": "2023-03-17T18:59:54.181285Z",
     "shell.execute_reply": "2023-03-17T18:59:54.179966Z",
     "shell.execute_reply.started": "2023-03-17T18:59:54.174583Z"
    },
    "id": "WES5hbPVWJht"
   },
   "outputs": [],
   "source": [
    "#data processing for segmenting sequence\n",
    "#trim_time can be adjusted to acquire different time length\n",
    "#250 500 750 1000\n",
    "\n",
    "def process_data(X_train_valid,y_train_valid,trim_time=500):   \n",
    "    x_train_valid_p, y_train_valid_p = data_prep(X_train_valid, y_train_valid, 2, 2, True,trim_time)\n",
    "    y_train_valid_p = to_categorical(y_train_valid_p, 4)\n",
    "    x_train_valid_p = x_train_valid_p.reshape(x_train_valid_p.shape[0], x_train_valid_p.shape[1], x_train_valid_p.shape[2], 1)\n",
    "    x_train_valid_p = np.swapaxes(x_train_valid_p, 1,3)\n",
    "    x_train_valid_p = np.swapaxes(x_train_valid_p, 1,2)\n",
    "    return x_train_valid_p,y_train_valid_p \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define basic CNN model and CNN-LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-17T19:00:02.219254Z",
     "iopub.status.busy": "2023-03-17T19:00:02.218220Z",
     "iopub.status.idle": "2023-03-17T19:00:02.231374Z",
     "shell.execute_reply": "2023-03-17T19:00:02.230241Z",
     "shell.execute_reply.started": "2023-03-17T19:00:02.219204Z"
    },
    "id": "XvrQaT3y4Iqn"
   },
   "outputs": [],
   "source": [
    "# Building the CNN model using sequential class\n",
    "def Basic_CNN(trim_time=500):\n",
    "  basic_cnn_model = Sequential()\n",
    "\n",
    "# Conv block 1 conv+maxpooling+batchnorm \n",
    "  basic_cnn_model.add(Conv2D(filters=10, kernel_size=(10,1), padding='same', activation='elu', input_shape=(int(trim_time/2),1,22)))\n",
    "  basic_cnn_model.add(MaxPooling2D(pool_size=(3,1), padding='same')) # Read the keras documentation\n",
    "  basic_cnn_model.add(BatchNormalization())\n",
    "  basic_cnn_model.add(Dropout(0.5))\n",
    "\n",
    "# Conv block 2\n",
    "  basic_cnn_model.add(Conv2D(filters=25, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "  basic_cnn_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "  basic_cnn_model.add(BatchNormalization())\n",
    "  basic_cnn_model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "# Conv Block 3\n",
    "  basic_cnn_model.add(Conv2D(filters=50, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "  basic_cnn_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "  basic_cnn_model.add(BatchNormalization())\n",
    "  basic_cnn_model.add(Dropout(0.5))\n",
    "\n",
    "# Conv Block 4\n",
    "  basic_cnn_model.add(Conv2D(filters=100, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "  basic_cnn_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "  basic_cnn_model.add(BatchNormalization())\n",
    "  basic_cnn_model.add(Dropout(0.5))\n",
    "\n",
    " # Conv Block 5\n",
    "  basic_cnn_model.add(Conv2D(filters=200, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "  basic_cnn_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "  basic_cnn_model.add(BatchNormalization())\n",
    "  basic_cnn_model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# FC layer + Softmax activation\n",
    "  basic_cnn_model.add(Flatten()) # Flattens the input\n",
    "  basic_cnn_model.add(Dense(4, activation='softmax')) # Output FC layer with softmax activation\n",
    "\n",
    "\n",
    "# Printing the model summary\n",
    "  basic_cnn_model.summary()\n",
    "  return basic_cnn_model\n",
    "\n",
    "#basic_cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-17T19:00:06.930431Z",
     "iopub.status.busy": "2023-03-17T19:00:06.929622Z",
     "iopub.status.idle": "2023-03-17T19:00:06.941039Z",
     "shell.execute_reply": "2023-03-17T19:00:06.939794Z",
     "shell.execute_reply.started": "2023-03-17T19:00:06.930393Z"
    }
   },
   "outputs": [],
   "source": [
    "def CNN_LSTM(dropout=0.5, units=150, add_layer = False,trim_time=500):\n",
    "  # Building the CNN model using sequential class\n",
    "  hybrid_cnn_lstm_model = Sequential()\n",
    "\n",
    "  # Conv. block 1\n",
    "  hybrid_cnn_lstm_model.add(Conv1D(filters=25, kernel_size=10, padding='same', activation='elu', input_shape=(int(trim_time/2),22)))\n",
    "  hybrid_cnn_lstm_model.add(MaxPooling1D(pool_size=3, padding='same')) # Read the keras documentation\n",
    "  hybrid_cnn_lstm_model.add(BatchNormalization())\n",
    "  hybrid_cnn_lstm_model.add(Dropout(0.5))\n",
    "\n",
    "  # Conv. block 2\n",
    "  hybrid_cnn_lstm_model.add(Conv1D(filters=50, kernel_size=10, padding='same', activation='elu'))\n",
    "  hybrid_cnn_lstm_model.add(MaxPooling1D(pool_size=3, padding='same'))\n",
    "  hybrid_cnn_lstm_model.add(BatchNormalization())\n",
    "  hybrid_cnn_lstm_model.add(Dropout(0.5))\n",
    "\n",
    "  # Conv. block 3\n",
    "  hybrid_cnn_lstm_model.add(Conv1D(filters=100, kernel_size=10, padding='same', activation='elu'))\n",
    "  hybrid_cnn_lstm_model.add(MaxPooling1D(pool_size=3, padding='same'))\n",
    "  hybrid_cnn_lstm_model.add(BatchNormalization())\n",
    "  hybrid_cnn_lstm_model.add(Dropout(0.5))\n",
    "\n",
    "  # Conv. block 4\n",
    "  hybrid_cnn_lstm_model.add(Conv1D(filters=200, kernel_size=10, padding='same', activation='elu'))\n",
    "  hybrid_cnn_lstm_model.add(MaxPooling1D(pool_size=3, padding='same'))\n",
    "  hybrid_cnn_lstm_model.add(BatchNormalization())\n",
    "  hybrid_cnn_lstm_model.add(Dropout(0.5))\n",
    "\n",
    "  # LSTM layer 1\n",
    "  hybrid_cnn_lstm_model.add(LSTM(units, dropout=dropout, recurrent_dropout=0, return_sequences=True))\n",
    "  if add_layer == True:\n",
    "    hybrid_cnn_lstm_model.add(LSTM(units, dropout=dropout, recurrent_dropout=0, return_sequences=True))\n",
    "  #LSTM layer 2\n",
    "  hybrid_cnn_lstm_model.add(LSTM(10, dropout=dropout, recurrent_dropout=0, return_sequences=False))\n",
    "  #FC layer\n",
    "  hybrid_cnn_lstm_model.add(Dense(4, activation='softmax')) # Output FC layer with softmax activation\n",
    "  return hybrid_cnn_lstm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## （1）Basic CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-17T09:00:25.820755Z",
     "iopub.status.busy": "2023-03-17T09:00:25.819994Z",
     "iopub.status.idle": "2023-03-17T09:09:26.721763Z",
     "shell.execute_reply": "2023-03-17T09:09:26.720415Z",
     "shell.execute_reply.started": "2023-03-17T09:00:25.820706Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X after trimming: (2115, 22, 250)\n",
      "Shape of X after maxpooling: (2115, 22, 125)\n",
      "Shape of X after averaging+noise and concatenating: (4230, 22, 125)\n",
      "Shape of X after subsampling and concatenating: (8460, 22, 125)\n",
      "Shape of X after trimming: (443, 22, 250)\n",
      "Shape of X after maxpooling: (443, 22, 125)\n",
      "Shape of X after averaging+noise and concatenating: (886, 22, 125)\n",
      "Shape of X after subsampling and concatenating: (1772, 22, 125)\n",
      "shape of train sequence (8460, 125, 1, 22)\n",
      "shape of train sequence label (8460, 4)\n",
      "shape of test sequence (1772, 125, 1, 22)\n",
      "shape of test sequence label (1772, 4)\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_10 (Conv2D)          (None, 125, 1, 10)        2210      \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPoolin  (None, 42, 1, 10)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 42, 1, 10)        40        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 42, 1, 10)         0         \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 42, 1, 25)         2525      \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPoolin  (None, 14, 1, 25)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 14, 1, 25)        100       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 14, 1, 25)         0         \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 14, 1, 50)         12550     \n",
      "                                                                 \n",
      " max_pooling2d_12 (MaxPoolin  (None, 5, 1, 50)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_12 (Bat  (None, 5, 1, 50)         200       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 5, 1, 50)          0         \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 5, 1, 100)         50100     \n",
      "                                                                 \n",
      " max_pooling2d_13 (MaxPoolin  (None, 2, 1, 100)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, 2, 1, 100)        400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 2, 1, 100)         0         \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 2, 1, 200)         200200    \n",
      "                                                                 \n",
      " max_pooling2d_14 (MaxPoolin  (None, 1, 1, 200)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_14 (Bat  (None, 1, 1, 200)        800       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 1, 1, 200)         0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 200)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 4)                 804       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 269,929\n",
      "Trainable params: 269,159\n",
      "Non-trainable params: 770\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "  9/133 [=>............................] - ETA: 0s - loss: 2.3725 - accuracy: 0.2240  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-17 09:00:28.576988: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_2/dropout_10/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 2s 6ms/step - loss: 2.0047 - accuracy: 0.2618\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.6284 - accuracy: 0.2728\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.4889 - accuracy: 0.2883\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.4306 - accuracy: 0.2998\n",
      "Epoch 5/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.3703 - accuracy: 0.3317\n",
      "Epoch 6/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 1.3350 - accuracy: 0.3606\n",
      "Epoch 7/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.2991 - accuracy: 0.3882\n",
      "Epoch 8/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.2705 - accuracy: 0.4061\n",
      "Epoch 9/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.2545 - accuracy: 0.4144\n",
      "Epoch 10/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.2390 - accuracy: 0.4310\n",
      "Epoch 11/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.2205 - accuracy: 0.4527\n",
      "Epoch 12/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.2072 - accuracy: 0.4520\n",
      "Epoch 13/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.1894 - accuracy: 0.4713\n",
      "Epoch 14/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.1813 - accuracy: 0.4723\n",
      "Epoch 15/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.1731 - accuracy: 0.4883\n",
      "Epoch 16/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 1.1475 - accuracy: 0.4987\n",
      "Epoch 17/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 1.1484 - accuracy: 0.5020\n",
      "Epoch 18/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.1344 - accuracy: 0.5082\n",
      "Epoch 19/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 1.1162 - accuracy: 0.5175\n",
      "Epoch 20/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.1105 - accuracy: 0.5271\n",
      "Epoch 21/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.0863 - accuracy: 0.5352\n",
      "Epoch 22/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.0756 - accuracy: 0.5457\n",
      "Epoch 23/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.0641 - accuracy: 0.5505\n",
      "Epoch 24/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.0606 - accuracy: 0.5546\n",
      "Epoch 25/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.0445 - accuracy: 0.5696\n",
      "Epoch 26/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.0378 - accuracy: 0.5662\n",
      "Epoch 27/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.0400 - accuracy: 0.5623\n",
      "Epoch 28/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.0260 - accuracy: 0.5727\n",
      "Epoch 29/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.0111 - accuracy: 0.5794\n",
      "Epoch 30/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.9967 - accuracy: 0.5824\n",
      "Epoch 31/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.9920 - accuracy: 0.5907\n",
      "Epoch 32/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.9806 - accuracy: 0.5947\n",
      "Epoch 33/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.9660 - accuracy: 0.5999\n",
      "Epoch 34/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.9745 - accuracy: 0.5972\n",
      "Epoch 35/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.9677 - accuracy: 0.6051\n",
      "Epoch 36/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.9448 - accuracy: 0.6097\n",
      "Epoch 37/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.9590 - accuracy: 0.6054\n",
      "Epoch 38/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.9556 - accuracy: 0.6091\n",
      "Epoch 39/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.9395 - accuracy: 0.6245\n",
      "Epoch 40/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.9301 - accuracy: 0.6262\n",
      "Epoch 41/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.9256 - accuracy: 0.6174\n",
      "Epoch 42/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.9128 - accuracy: 0.6275\n",
      "Epoch 43/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.9110 - accuracy: 0.6288\n",
      "Epoch 44/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.9130 - accuracy: 0.6248\n",
      "Epoch 45/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.8942 - accuracy: 0.6405\n",
      "Epoch 46/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.8962 - accuracy: 0.6403\n",
      "Epoch 47/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.9000 - accuracy: 0.6363\n",
      "Epoch 48/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8834 - accuracy: 0.6437\n",
      "Epoch 49/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8969 - accuracy: 0.6370\n",
      "Epoch 50/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8868 - accuracy: 0.6350\n",
      "Epoch 51/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8863 - accuracy: 0.6369\n",
      "Epoch 52/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8761 - accuracy: 0.6434\n",
      "Epoch 53/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8764 - accuracy: 0.6455\n",
      "Epoch 54/100\n",
      "133/133 [==============================] - 1s 9ms/step - loss: 0.8744 - accuracy: 0.6441\n",
      "Epoch 55/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8633 - accuracy: 0.6524\n",
      "Epoch 56/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8672 - accuracy: 0.6474\n",
      "Epoch 57/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8612 - accuracy: 0.6521\n",
      "Epoch 58/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.8552 - accuracy: 0.6539\n",
      "Epoch 59/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.8478 - accuracy: 0.6609\n",
      "Epoch 60/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8523 - accuracy: 0.6583\n",
      "Epoch 61/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8513 - accuracy: 0.6630\n",
      "Epoch 62/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8520 - accuracy: 0.6563\n",
      "Epoch 63/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.8449 - accuracy: 0.6647\n",
      "Epoch 64/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8393 - accuracy: 0.6644\n",
      "Epoch 65/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8322 - accuracy: 0.6658\n",
      "Epoch 66/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8275 - accuracy: 0.6643\n",
      "Epoch 67/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8192 - accuracy: 0.6727\n",
      "Epoch 68/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8324 - accuracy: 0.6654\n",
      "Epoch 69/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8270 - accuracy: 0.6693\n",
      "Epoch 70/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8335 - accuracy: 0.6684\n",
      "Epoch 71/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8233 - accuracy: 0.6691\n",
      "Epoch 72/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.8221 - accuracy: 0.6671\n",
      "Epoch 73/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8120 - accuracy: 0.6728\n",
      "Epoch 74/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8103 - accuracy: 0.6800\n",
      "Epoch 75/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8072 - accuracy: 0.6790\n",
      "Epoch 76/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8118 - accuracy: 0.6761\n",
      "Epoch 77/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8139 - accuracy: 0.6726\n",
      "Epoch 78/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8119 - accuracy: 0.6752\n",
      "Epoch 79/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8019 - accuracy: 0.6792\n",
      "Epoch 80/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8007 - accuracy: 0.6758\n",
      "Epoch 81/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8000 - accuracy: 0.6786\n",
      "Epoch 82/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8112 - accuracy: 0.6761\n",
      "Epoch 83/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7880 - accuracy: 0.6879\n",
      "Epoch 84/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7988 - accuracy: 0.6786\n",
      "Epoch 85/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.7853 - accuracy: 0.6907\n",
      "Epoch 86/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7908 - accuracy: 0.6879\n",
      "Epoch 87/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7892 - accuracy: 0.6835\n",
      "Epoch 88/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7923 - accuracy: 0.6832\n",
      "Epoch 89/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7914 - accuracy: 0.6849\n",
      "Epoch 90/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7868 - accuracy: 0.6868\n",
      "Epoch 91/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.7842 - accuracy: 0.6874\n",
      "Epoch 92/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.7826 - accuracy: 0.6911\n",
      "Epoch 93/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7770 - accuracy: 0.6911\n",
      "Epoch 94/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7710 - accuracy: 0.6897\n",
      "Epoch 95/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7822 - accuracy: 0.6826\n",
      "Epoch 96/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7717 - accuracy: 0.6929\n",
      "Epoch 97/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7796 - accuracy: 0.6980\n",
      "Epoch 98/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.7640 - accuracy: 0.6987\n",
      "Epoch 99/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7721 - accuracy: 0.6926\n",
      "Epoch 100/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7732 - accuracy: 0.6920\n",
      "Test accuracy of the basic CNN model: 0.6788939237594604\n",
      "Shape of X after trimming: (2115, 22, 500)\n",
      "Shape of X after maxpooling: (2115, 22, 250)\n",
      "Shape of X after averaging+noise and concatenating: (4230, 22, 250)\n",
      "Shape of X after subsampling and concatenating: (8460, 22, 250)\n",
      "Shape of X after trimming: (443, 22, 500)\n",
      "Shape of X after maxpooling: (443, 22, 250)\n",
      "Shape of X after averaging+noise and concatenating: (886, 22, 250)\n",
      "Shape of X after subsampling and concatenating: (1772, 22, 250)\n",
      "shape of train sequence (8460, 250, 1, 22)\n",
      "shape of train sequence label (8460, 4)\n",
      "shape of test sequence (1772, 250, 1, 22)\n",
      "shape of test sequence label (1772, 4)\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_15 (Conv2D)          (None, 250, 1, 10)        2210      \n",
      "                                                                 \n",
      " max_pooling2d_15 (MaxPoolin  (None, 84, 1, 10)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_15 (Bat  (None, 84, 1, 10)        40        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 84, 1, 10)         0         \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 84, 1, 25)         2525      \n",
      "                                                                 \n",
      " max_pooling2d_16 (MaxPoolin  (None, 28, 1, 25)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_16 (Bat  (None, 28, 1, 25)        100       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 28, 1, 25)         0         \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 28, 1, 50)         12550     \n",
      "                                                                 \n",
      " max_pooling2d_17 (MaxPoolin  (None, 10, 1, 50)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_17 (Bat  (None, 10, 1, 50)        200       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 10, 1, 50)         0         \n",
      "                                                                 \n",
      " conv2d_18 (Conv2D)          (None, 10, 1, 100)        50100     \n",
      "                                                                 \n",
      " max_pooling2d_18 (MaxPoolin  (None, 4, 1, 100)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_18 (Bat  (None, 4, 1, 100)        400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, 4, 1, 100)         0         \n",
      "                                                                 \n",
      " conv2d_19 (Conv2D)          (None, 4, 1, 200)         200200    \n",
      "                                                                 \n",
      " max_pooling2d_19 (MaxPoolin  (None, 2, 1, 200)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_19 (Bat  (None, 2, 1, 200)        800       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (None, 2, 1, 200)         0         \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 400)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 4)                 1604      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 270,729\n",
      "Trainable params: 269,959\n",
      "Non-trainable params: 770\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "  8/133 [>.............................] - ETA: 0s - loss: 2.4790 - accuracy: 0.2578  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-17 09:02:55.229083: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_3/dropout_15/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 2s 7ms/step - loss: 2.0269 - accuracy: 0.2762\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 1.5645 - accuracy: 0.3164\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.4178 - accuracy: 0.3526\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.3069 - accuracy: 0.4054\n",
      "Epoch 5/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 1.2445 - accuracy: 0.4345\n",
      "Epoch 6/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.2181 - accuracy: 0.4495\n",
      "Epoch 7/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.1755 - accuracy: 0.4839\n",
      "Epoch 8/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 1.1483 - accuracy: 0.4991\n",
      "Epoch 9/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 1.1336 - accuracy: 0.5115\n",
      "Epoch 10/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 1.1139 - accuracy: 0.5241\n",
      "Epoch 11/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 1.0829 - accuracy: 0.5461\n",
      "Epoch 12/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.0596 - accuracy: 0.5579\n",
      "Epoch 13/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.0349 - accuracy: 0.5714\n",
      "Epoch 14/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 1.0075 - accuracy: 0.5810\n",
      "Epoch 15/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 1.0024 - accuracy: 0.5846\n",
      "Epoch 16/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.9752 - accuracy: 0.5983\n",
      "Epoch 17/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.9735 - accuracy: 0.5992\n",
      "Epoch 18/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.9507 - accuracy: 0.6115\n",
      "Epoch 19/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.9313 - accuracy: 0.6194\n",
      "Epoch 20/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.9315 - accuracy: 0.6212\n",
      "Epoch 21/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.9055 - accuracy: 0.6345\n",
      "Epoch 22/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8868 - accuracy: 0.6390\n",
      "Epoch 23/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8836 - accuracy: 0.6455\n",
      "Epoch 24/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8649 - accuracy: 0.6528\n",
      "Epoch 25/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.8619 - accuracy: 0.6553\n",
      "Epoch 26/100\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.8566 - accuracy: 0.6626\n",
      "Epoch 27/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.8460 - accuracy: 0.6609\n",
      "Epoch 28/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8212 - accuracy: 0.6714\n",
      "Epoch 29/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8216 - accuracy: 0.6761\n",
      "Epoch 30/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8156 - accuracy: 0.6755\n",
      "Epoch 31/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8049 - accuracy: 0.6793\n",
      "Epoch 32/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7975 - accuracy: 0.6830\n",
      "Epoch 33/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8080 - accuracy: 0.6799\n",
      "Epoch 34/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7679 - accuracy: 0.6956\n",
      "Epoch 35/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7749 - accuracy: 0.6956\n",
      "Epoch 36/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.7752 - accuracy: 0.6965\n",
      "Epoch 37/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.7645 - accuracy: 0.6968\n",
      "Epoch 38/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.7417 - accuracy: 0.7058\n",
      "Epoch 39/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.7486 - accuracy: 0.7061\n",
      "Epoch 40/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7407 - accuracy: 0.7058\n",
      "Epoch 41/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7383 - accuracy: 0.7104\n",
      "Epoch 42/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7205 - accuracy: 0.7162\n",
      "Epoch 43/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.7403 - accuracy: 0.7105\n",
      "Epoch 44/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7359 - accuracy: 0.7106\n",
      "Epoch 45/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7256 - accuracy: 0.7135\n",
      "Epoch 46/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7152 - accuracy: 0.7152\n",
      "Epoch 47/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6966 - accuracy: 0.7274\n",
      "Epoch 48/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7224 - accuracy: 0.7188\n",
      "Epoch 49/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6993 - accuracy: 0.7272\n",
      "Epoch 50/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.6947 - accuracy: 0.7336\n",
      "Epoch 51/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.6933 - accuracy: 0.7265\n",
      "Epoch 52/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6758 - accuracy: 0.7370\n",
      "Epoch 53/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6948 - accuracy: 0.7272\n",
      "Epoch 54/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6720 - accuracy: 0.7361\n",
      "Epoch 55/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.6685 - accuracy: 0.7348\n",
      "Epoch 56/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6736 - accuracy: 0.7350\n",
      "Epoch 57/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6640 - accuracy: 0.7377\n",
      "Epoch 58/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6580 - accuracy: 0.7421\n",
      "Epoch 59/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6630 - accuracy: 0.7411\n",
      "Epoch 60/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6570 - accuracy: 0.7416\n",
      "Epoch 61/100\n",
      "133/133 [==============================] - 1s 10ms/step - loss: 0.6514 - accuracy: 0.7469\n",
      "Epoch 62/100\n",
      "133/133 [==============================] - 1s 9ms/step - loss: 0.6393 - accuracy: 0.7515\n",
      "Epoch 63/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.6422 - accuracy: 0.7486\n",
      "Epoch 64/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6374 - accuracy: 0.7554\n",
      "Epoch 65/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6335 - accuracy: 0.7526\n",
      "Epoch 66/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.6373 - accuracy: 0.7538\n",
      "Epoch 67/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.6425 - accuracy: 0.7491\n",
      "Epoch 68/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6406 - accuracy: 0.7508\n",
      "Epoch 69/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.6416 - accuracy: 0.7517\n",
      "Epoch 70/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6237 - accuracy: 0.7587\n",
      "Epoch 71/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.6235 - accuracy: 0.7544\n",
      "Epoch 72/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.6277 - accuracy: 0.7577\n",
      "Epoch 73/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6158 - accuracy: 0.7578\n",
      "Epoch 74/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6120 - accuracy: 0.7611\n",
      "Epoch 75/100\n",
      "133/133 [==============================] - 1s 9ms/step - loss: 0.6278 - accuracy: 0.7546\n",
      "Epoch 76/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.5983 - accuracy: 0.7626\n",
      "Epoch 77/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6187 - accuracy: 0.7602\n",
      "Epoch 78/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6112 - accuracy: 0.7596\n",
      "Epoch 79/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.5938 - accuracy: 0.7675\n",
      "Epoch 80/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6093 - accuracy: 0.7662\n",
      "Epoch 81/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6080 - accuracy: 0.7590\n",
      "Epoch 82/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.5878 - accuracy: 0.7715\n",
      "Epoch 83/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.5904 - accuracy: 0.7741\n",
      "Epoch 84/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.5786 - accuracy: 0.7758\n",
      "Epoch 85/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.5974 - accuracy: 0.7652\n",
      "Epoch 86/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.5985 - accuracy: 0.7677\n",
      "Epoch 87/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.6014 - accuracy: 0.7670\n",
      "Epoch 88/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.5873 - accuracy: 0.7708\n",
      "Epoch 89/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.5922 - accuracy: 0.7702\n",
      "Epoch 90/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.5891 - accuracy: 0.7717\n",
      "Epoch 91/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.5727 - accuracy: 0.7757\n",
      "Epoch 92/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.5931 - accuracy: 0.7709\n",
      "Epoch 93/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.5771 - accuracy: 0.7751\n",
      "Epoch 94/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.5787 - accuracy: 0.7799\n",
      "Epoch 95/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.5742 - accuracy: 0.7747\n",
      "Epoch 96/100\n",
      "133/133 [==============================] - 1s 9ms/step - loss: 0.5614 - accuracy: 0.7823\n",
      "Epoch 97/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.5565 - accuracy: 0.7833\n",
      "Epoch 98/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.5669 - accuracy: 0.7814\n",
      "Epoch 99/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.5705 - accuracy: 0.7778\n",
      "Epoch 100/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.5697 - accuracy: 0.7818\n",
      "Test accuracy of the basic CNN model: 0.72516930103302\n",
      "Shape of X after trimming: (2115, 22, 750)\n",
      "Shape of X after maxpooling: (2115, 22, 375)\n",
      "Shape of X after averaging+noise and concatenating: (4230, 22, 375)\n",
      "Shape of X after subsampling and concatenating: (8460, 22, 375)\n",
      "Shape of X after trimming: (443, 22, 750)\n",
      "Shape of X after maxpooling: (443, 22, 375)\n",
      "Shape of X after averaging+noise and concatenating: (886, 22, 375)\n",
      "Shape of X after subsampling and concatenating: (1772, 22, 375)\n",
      "shape of train sequence (8460, 375, 1, 22)\n",
      "shape of train sequence label (8460, 4)\n",
      "shape of test sequence (1772, 375, 1, 22)\n",
      "shape of test sequence label (1772, 4)\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_20 (Conv2D)          (None, 375, 1, 10)        2210      \n",
      "                                                                 \n",
      " max_pooling2d_20 (MaxPoolin  (None, 125, 1, 10)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_20 (Bat  (None, 125, 1, 10)       40        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, 125, 1, 10)        0         \n",
      "                                                                 \n",
      " conv2d_21 (Conv2D)          (None, 125, 1, 25)        2525      \n",
      "                                                                 \n",
      " max_pooling2d_21 (MaxPoolin  (None, 42, 1, 25)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_21 (Bat  (None, 42, 1, 25)        100       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (None, 42, 1, 25)         0         \n",
      "                                                                 \n",
      " conv2d_22 (Conv2D)          (None, 42, 1, 50)         12550     \n",
      "                                                                 \n",
      " max_pooling2d_22 (MaxPoolin  (None, 14, 1, 50)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_22 (Bat  (None, 14, 1, 50)        200       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (None, 14, 1, 50)         0         \n",
      "                                                                 \n",
      " conv2d_23 (Conv2D)          (None, 14, 1, 100)        50100     \n",
      "                                                                 \n",
      " max_pooling2d_23 (MaxPoolin  (None, 5, 1, 100)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_23 (Bat  (None, 5, 1, 100)        400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (None, 5, 1, 100)         0         \n",
      "                                                                 \n",
      " conv2d_24 (Conv2D)          (None, 5, 1, 200)         200200    \n",
      "                                                                 \n",
      " max_pooling2d_24 (MaxPoolin  (None, 2, 1, 200)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_24 (Bat  (None, 2, 1, 200)        800       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, 2, 1, 200)         0         \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 400)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 4)                 1604      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 270,729\n",
      "Trainable params: 269,959\n",
      "Non-trainable params: 770\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-17 09:04:32.087018: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_4/dropout_20/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 2s 8ms/step - loss: 2.0095 - accuracy: 0.2631\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 1.5723 - accuracy: 0.3022\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 1.4482 - accuracy: 0.3214\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 1.3689 - accuracy: 0.3564\n",
      "Epoch 5/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 1.3057 - accuracy: 0.3949\n",
      "Epoch 6/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 1.2696 - accuracy: 0.4137\n",
      "Epoch 7/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 1.2307 - accuracy: 0.4339\n",
      "Epoch 8/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 1.2126 - accuracy: 0.4449\n",
      "Epoch 9/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 1.1892 - accuracy: 0.4671\n",
      "Epoch 10/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 1.1678 - accuracy: 0.4843\n",
      "Epoch 11/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 1.1311 - accuracy: 0.5116\n",
      "Epoch 12/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 1.1120 - accuracy: 0.5212\n",
      "Epoch 13/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 1.0892 - accuracy: 0.5392\n",
      "Epoch 14/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 1.0632 - accuracy: 0.5528\n",
      "Epoch 15/100\n",
      "133/133 [==============================] - 1s 9ms/step - loss: 1.0461 - accuracy: 0.5526\n",
      "Epoch 16/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 1.0173 - accuracy: 0.5746\n",
      "Epoch 17/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.9835 - accuracy: 0.5896\n",
      "Epoch 18/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.9706 - accuracy: 0.5982\n",
      "Epoch 19/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.9368 - accuracy: 0.6173\n",
      "Epoch 20/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.9095 - accuracy: 0.6272\n",
      "Epoch 21/100\n",
      "133/133 [==============================] - 1s 10ms/step - loss: 0.9062 - accuracy: 0.6303\n",
      "Epoch 22/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.8871 - accuracy: 0.6414\n",
      "Epoch 23/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.8557 - accuracy: 0.6509\n",
      "Epoch 24/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.8733 - accuracy: 0.6459\n",
      "Epoch 25/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.8422 - accuracy: 0.6657\n",
      "Epoch 26/100\n",
      "133/133 [==============================] - 1s 9ms/step - loss: 0.8326 - accuracy: 0.6623\n",
      "Epoch 27/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.8298 - accuracy: 0.6674\n",
      "Epoch 28/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.7986 - accuracy: 0.6780\n",
      "Epoch 29/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.8039 - accuracy: 0.6811\n",
      "Epoch 30/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.7968 - accuracy: 0.6777\n",
      "Epoch 31/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.7740 - accuracy: 0.6863\n",
      "Epoch 32/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.7693 - accuracy: 0.6905\n",
      "Epoch 33/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.7471 - accuracy: 0.7034\n",
      "Epoch 34/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.7519 - accuracy: 0.6935\n",
      "Epoch 35/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.7338 - accuracy: 0.7046\n",
      "Epoch 36/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.7376 - accuracy: 0.7059\n",
      "Epoch 37/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.7293 - accuracy: 0.7129\n",
      "Epoch 38/100\n",
      "133/133 [==============================] - 1s 9ms/step - loss: 0.7136 - accuracy: 0.7164\n",
      "Epoch 39/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.7147 - accuracy: 0.7178\n",
      "Epoch 40/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.7120 - accuracy: 0.7148\n",
      "Epoch 41/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.6953 - accuracy: 0.7242\n",
      "Epoch 42/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.6924 - accuracy: 0.7296\n",
      "Epoch 43/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.6881 - accuracy: 0.7253\n",
      "Epoch 44/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.6703 - accuracy: 0.7319\n",
      "Epoch 45/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.6684 - accuracy: 0.7375\n",
      "Epoch 46/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.6790 - accuracy: 0.7324\n",
      "Epoch 47/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.6848 - accuracy: 0.7278\n",
      "Epoch 48/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.6500 - accuracy: 0.7461\n",
      "Epoch 49/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.6370 - accuracy: 0.7524\n",
      "Epoch 50/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.6500 - accuracy: 0.7424\n",
      "Epoch 51/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.6458 - accuracy: 0.7492\n",
      "Epoch 52/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.6350 - accuracy: 0.7514\n",
      "Epoch 53/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.6401 - accuracy: 0.7515\n",
      "Epoch 54/100\n",
      "133/133 [==============================] - 1s 9ms/step - loss: 0.6284 - accuracy: 0.7512\n",
      "Epoch 55/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.6199 - accuracy: 0.7651\n",
      "Epoch 56/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.6243 - accuracy: 0.7576\n",
      "Epoch 57/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.6060 - accuracy: 0.7657\n",
      "Epoch 58/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.6138 - accuracy: 0.7605\n",
      "Epoch 59/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.6090 - accuracy: 0.7610\n",
      "Epoch 60/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.6033 - accuracy: 0.7611\n",
      "Epoch 61/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.5985 - accuracy: 0.7623\n",
      "Epoch 62/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.6074 - accuracy: 0.7628\n",
      "Epoch 63/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.5809 - accuracy: 0.7722\n",
      "Epoch 64/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.5950 - accuracy: 0.7689\n",
      "Epoch 65/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.5893 - accuracy: 0.7702\n",
      "Epoch 66/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.5829 - accuracy: 0.7730\n",
      "Epoch 67/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.5863 - accuracy: 0.7690\n",
      "Epoch 68/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.5713 - accuracy: 0.7792\n",
      "Epoch 69/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.5781 - accuracy: 0.7757\n",
      "Epoch 70/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.5713 - accuracy: 0.7689\n",
      "Epoch 71/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.5588 - accuracy: 0.7852\n",
      "Epoch 72/100\n",
      "133/133 [==============================] - 1s 9ms/step - loss: 0.5649 - accuracy: 0.7778\n",
      "Epoch 73/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.5554 - accuracy: 0.7779\n",
      "Epoch 74/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.5397 - accuracy: 0.7918\n",
      "Epoch 75/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.5490 - accuracy: 0.7875\n",
      "Epoch 76/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.5461 - accuracy: 0.7852\n",
      "Epoch 77/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.5612 - accuracy: 0.7851\n",
      "Epoch 78/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.5495 - accuracy: 0.7804\n",
      "Epoch 79/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.5355 - accuracy: 0.7922\n",
      "Epoch 80/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.5380 - accuracy: 0.7921\n",
      "Epoch 81/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.5404 - accuracy: 0.7922\n",
      "Epoch 82/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.5314 - accuracy: 0.7972\n",
      "Epoch 83/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.5366 - accuracy: 0.7933\n",
      "Epoch 84/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.5433 - accuracy: 0.7853\n",
      "Epoch 85/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.5274 - accuracy: 0.7968\n",
      "Epoch 86/100\n",
      "133/133 [==============================] - 1s 10ms/step - loss: 0.5295 - accuracy: 0.7975\n",
      "Epoch 87/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.5047 - accuracy: 0.8054\n",
      "Epoch 88/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.5234 - accuracy: 0.7937\n",
      "Epoch 89/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.5278 - accuracy: 0.7962\n",
      "Epoch 90/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.5247 - accuracy: 0.7978\n",
      "Epoch 91/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.5154 - accuracy: 0.8033\n",
      "Epoch 92/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.4975 - accuracy: 0.8056\n",
      "Epoch 93/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.5031 - accuracy: 0.8103\n",
      "Epoch 94/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.5016 - accuracy: 0.7998\n",
      "Epoch 95/100\n",
      "133/133 [==============================] - 1s 9ms/step - loss: 0.5213 - accuracy: 0.7965\n",
      "Epoch 96/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.5187 - accuracy: 0.8001\n",
      "Epoch 97/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.4975 - accuracy: 0.8105\n",
      "Epoch 98/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.5082 - accuracy: 0.8044\n",
      "Epoch 99/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.4950 - accuracy: 0.8091\n",
      "Epoch 100/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.4896 - accuracy: 0.8083\n",
      "Test accuracy of the basic CNN model: 0.6890519261360168\n",
      "Shape of X after trimming: (2115, 22, 1000)\n",
      "Shape of X after maxpooling: (2115, 22, 500)\n",
      "Shape of X after averaging+noise and concatenating: (4230, 22, 500)\n",
      "Shape of X after subsampling and concatenating: (8460, 22, 500)\n",
      "Shape of X after trimming: (443, 22, 1000)\n",
      "Shape of X after maxpooling: (443, 22, 500)\n",
      "Shape of X after averaging+noise and concatenating: (886, 22, 500)\n",
      "Shape of X after subsampling and concatenating: (1772, 22, 500)\n",
      "shape of train sequence (8460, 500, 1, 22)\n",
      "shape of train sequence label (8460, 4)\n",
      "shape of test sequence (1772, 500, 1, 22)\n",
      "shape of test sequence label (1772, 4)\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_25 (Conv2D)          (None, 500, 1, 10)        2210      \n",
      "                                                                 \n",
      " max_pooling2d_25 (MaxPoolin  (None, 167, 1, 10)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_25 (Bat  (None, 167, 1, 10)       40        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_25 (Dropout)        (None, 167, 1, 10)        0         \n",
      "                                                                 \n",
      " conv2d_26 (Conv2D)          (None, 167, 1, 25)        2525      \n",
      "                                                                 \n",
      " max_pooling2d_26 (MaxPoolin  (None, 56, 1, 25)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_26 (Bat  (None, 56, 1, 25)        100       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_26 (Dropout)        (None, 56, 1, 25)         0         \n",
      "                                                                 \n",
      " conv2d_27 (Conv2D)          (None, 56, 1, 50)         12550     \n",
      "                                                                 \n",
      " max_pooling2d_27 (MaxPoolin  (None, 19, 1, 50)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_27 (Bat  (None, 19, 1, 50)        200       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_27 (Dropout)        (None, 19, 1, 50)         0         \n",
      "                                                                 \n",
      " conv2d_28 (Conv2D)          (None, 19, 1, 100)        50100     \n",
      "                                                                 \n",
      " max_pooling2d_28 (MaxPoolin  (None, 7, 1, 100)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_28 (Bat  (None, 7, 1, 100)        400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_28 (Dropout)        (None, 7, 1, 100)         0         \n",
      "                                                                 \n",
      " conv2d_29 (Conv2D)          (None, 7, 1, 200)         200200    \n",
      "                                                                 \n",
      " max_pooling2d_29 (MaxPoolin  (None, 3, 1, 200)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_29 (Bat  (None, 3, 1, 200)        800       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_29 (Dropout)        (None, 3, 1, 200)         0         \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 600)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 4)                 2404      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 271,529\n",
      "Trainable params: 270,759\n",
      "Non-trainable params: 770\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-17 09:07:03.643182: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_5/dropout_25/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 3s 10ms/step - loss: 2.0402 - accuracy: 0.2738\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 1.6129 - accuracy: 0.2941\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 1.4698 - accuracy: 0.3271\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 1.3824 - accuracy: 0.3550\n",
      "Epoch 5/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 1.3184 - accuracy: 0.3849\n",
      "Epoch 6/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 1.2622 - accuracy: 0.4203\n",
      "Epoch 7/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 1.2252 - accuracy: 0.4492\n",
      "Epoch 8/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 1.2098 - accuracy: 0.4508\n",
      "Epoch 9/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 1.1749 - accuracy: 0.4755\n",
      "Epoch 10/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 1.1598 - accuracy: 0.4819\n",
      "Epoch 11/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 1.1420 - accuracy: 0.4989\n",
      "Epoch 12/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 1.1013 - accuracy: 0.5226\n",
      "Epoch 13/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 1.0695 - accuracy: 0.5429\n",
      "Epoch 14/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 1.0485 - accuracy: 0.5598\n",
      "Epoch 15/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 1.0140 - accuracy: 0.5788\n",
      "Epoch 16/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 1.0133 - accuracy: 0.5838\n",
      "Epoch 17/100\n",
      "133/133 [==============================] - 1s 9ms/step - loss: 0.9870 - accuracy: 0.5968\n",
      "Epoch 18/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.9663 - accuracy: 0.6059\n",
      "Epoch 19/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.9460 - accuracy: 0.6096\n",
      "Epoch 20/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.9168 - accuracy: 0.6274\n",
      "Epoch 21/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.9012 - accuracy: 0.6284\n",
      "Epoch 22/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.8897 - accuracy: 0.6413\n",
      "Epoch 23/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.8593 - accuracy: 0.6559\n",
      "Epoch 24/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.8568 - accuracy: 0.6513\n",
      "Epoch 25/100\n",
      "133/133 [==============================] - 1s 10ms/step - loss: 0.8323 - accuracy: 0.6675\n",
      "Epoch 26/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.8134 - accuracy: 0.6716\n",
      "Epoch 27/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.8097 - accuracy: 0.6760\n",
      "Epoch 28/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.7939 - accuracy: 0.6813\n",
      "Epoch 29/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.7796 - accuracy: 0.6898\n",
      "Epoch 30/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.7639 - accuracy: 0.6952\n",
      "Epoch 31/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.7549 - accuracy: 0.7006\n",
      "Epoch 32/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.7398 - accuracy: 0.7041\n",
      "Epoch 33/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.7357 - accuracy: 0.7067\n",
      "Epoch 34/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.7181 - accuracy: 0.7193\n",
      "Epoch 35/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.7107 - accuracy: 0.7174\n",
      "Epoch 36/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.7283 - accuracy: 0.7178\n",
      "Epoch 37/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.6989 - accuracy: 0.7214\n",
      "Epoch 38/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.6873 - accuracy: 0.7275\n",
      "Epoch 39/100\n",
      "133/133 [==============================] - 1s 9ms/step - loss: 0.6890 - accuracy: 0.7272\n",
      "Epoch 40/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.6752 - accuracy: 0.7322\n",
      "Epoch 41/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.6611 - accuracy: 0.7351\n",
      "Epoch 42/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.6615 - accuracy: 0.7434\n",
      "Epoch 43/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.6624 - accuracy: 0.7418\n",
      "Epoch 44/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.6462 - accuracy: 0.7485\n",
      "Epoch 45/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.6334 - accuracy: 0.7567\n",
      "Epoch 46/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.6347 - accuracy: 0.7539\n",
      "Epoch 47/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.6156 - accuracy: 0.7586\n",
      "Epoch 48/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.6117 - accuracy: 0.7637\n",
      "Epoch 49/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.6033 - accuracy: 0.7642\n",
      "Epoch 50/100\n",
      "133/133 [==============================] - 1s 9ms/step - loss: 0.6108 - accuracy: 0.7631\n",
      "Epoch 51/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.5979 - accuracy: 0.7600\n",
      "Epoch 52/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.6008 - accuracy: 0.7596\n",
      "Epoch 53/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.5736 - accuracy: 0.7735\n",
      "Epoch 54/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.5853 - accuracy: 0.7749\n",
      "Epoch 55/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.5743 - accuracy: 0.7839\n",
      "Epoch 56/100\n",
      "133/133 [==============================] - 1s 9ms/step - loss: 0.5649 - accuracy: 0.7813\n",
      "Epoch 57/100\n",
      "133/133 [==============================] - 1s 9ms/step - loss: 0.5620 - accuracy: 0.7799\n",
      "Epoch 58/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.5530 - accuracy: 0.7884\n",
      "Epoch 59/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.5719 - accuracy: 0.7773\n",
      "Epoch 60/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.5546 - accuracy: 0.7814\n",
      "Epoch 61/100\n",
      "133/133 [==============================] - 1s 9ms/step - loss: 0.5505 - accuracy: 0.7829\n",
      "Epoch 62/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.5460 - accuracy: 0.7875\n",
      "Epoch 63/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.5289 - accuracy: 0.7941\n",
      "Epoch 64/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.5154 - accuracy: 0.7993\n",
      "Epoch 65/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.5099 - accuracy: 0.8030\n",
      "Epoch 66/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.5334 - accuracy: 0.7974\n",
      "Epoch 67/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.5320 - accuracy: 0.7939\n",
      "Epoch 68/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.5088 - accuracy: 0.8025\n",
      "Epoch 69/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.5127 - accuracy: 0.8030\n",
      "Epoch 70/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.5173 - accuracy: 0.8000\n",
      "Epoch 71/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.4845 - accuracy: 0.8152\n",
      "Epoch 72/100\n",
      "133/133 [==============================] - 1s 9ms/step - loss: 0.5006 - accuracy: 0.8104\n",
      "Epoch 73/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.4979 - accuracy: 0.8060\n",
      "Epoch 74/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.4910 - accuracy: 0.8097\n",
      "Epoch 75/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.4962 - accuracy: 0.8079\n",
      "Epoch 76/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.5069 - accuracy: 0.8030\n",
      "Epoch 77/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.4851 - accuracy: 0.8175\n",
      "Epoch 78/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.4801 - accuracy: 0.8158\n",
      "Epoch 79/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.4803 - accuracy: 0.8194\n",
      "Epoch 80/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.4911 - accuracy: 0.8110\n",
      "Epoch 81/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.4656 - accuracy: 0.8220\n",
      "Epoch 82/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.4725 - accuracy: 0.8162\n",
      "Epoch 83/100\n",
      "133/133 [==============================] - 1s 9ms/step - loss: 0.4656 - accuracy: 0.8187\n",
      "Epoch 84/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.4667 - accuracy: 0.8200\n",
      "Epoch 85/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.4638 - accuracy: 0.8210\n",
      "Epoch 86/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.4667 - accuracy: 0.8225\n",
      "Epoch 87/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.4622 - accuracy: 0.8253\n",
      "Epoch 88/100\n",
      "133/133 [==============================] - 1s 10ms/step - loss: 0.4532 - accuracy: 0.8265\n",
      "Epoch 89/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.4576 - accuracy: 0.8299\n",
      "Epoch 90/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.4646 - accuracy: 0.8240\n",
      "Epoch 91/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.4529 - accuracy: 0.8279\n",
      "Epoch 92/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.4434 - accuracy: 0.8293\n",
      "Epoch 93/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.4430 - accuracy: 0.8304\n",
      "Epoch 94/100\n",
      "133/133 [==============================] - 1s 9ms/step - loss: 0.4394 - accuracy: 0.8382\n",
      "Epoch 95/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.4380 - accuracy: 0.8316\n",
      "Epoch 96/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.4366 - accuracy: 0.8333\n",
      "Epoch 97/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.4386 - accuracy: 0.8278\n",
      "Epoch 98/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.4325 - accuracy: 0.8364\n",
      "Epoch 99/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.4373 - accuracy: 0.8339\n",
      "Epoch 100/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.4308 - accuracy: 0.8378\n",
      "Test accuracy of the basic CNN model: 0.6681715846061707\n"
     ]
    }
   ],
   "source": [
    "time_sequence=[250,500,750,1000] #use 250 500 750 1000 time bins respectively\n",
    "CNN_time_accuracy=[]\n",
    "for trim_time in time_sequence:\n",
    "    X_train_sequence,y_train_sequence=process_data(X_train_valid,y_train_valid,trim_time=trim_time)\n",
    "    X_test_sequence,y_test_sequence=process_data(X_test,y_test_original,trim_time=trim_time)\n",
    "    print('shape of train sequence',np.shape(X_train_sequence))\n",
    "    print('shape of train sequence label',np.shape(y_train_sequence))\n",
    "    print('shape of test sequence',np.shape(X_test_sequence))\n",
    "    print('shape of test sequence label',np.shape(y_test_sequence))\n",
    "    learning_rate = 1e-3\n",
    "    epochs=100\n",
    "    cnn_optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    basic_cnn_model=Basic_CNN(trim_time=trim_time)\n",
    "    basic_cnn_model.compile(loss='categorical_crossentropy',\n",
    "                 optimizer=cnn_optimizer,\n",
    "                 metrics=['accuracy'])\n",
    "    cnn_model_results = basic_cnn_model.fit(X_train_sequence,y_train_sequence,\n",
    "             batch_size=64,\n",
    "             epochs=epochs,verbose=True)\n",
    "    cnn_score= basic_cnn_model.evaluate(X_test_sequence,y_test_sequence,verbose=0)\n",
    "    CNN_time_accuracy.append(cnn_score[1])\n",
    "\n",
    "    print('Test accuracy of the basic CNN model:',cnn_score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) CNN_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-17T19:46:02.145017Z",
     "iopub.status.busy": "2023-03-17T19:46:02.144025Z",
     "iopub.status.idle": "2023-03-17T20:00:14.693596Z",
     "shell.execute_reply": "2023-03-17T20:00:14.692562Z",
     "shell.execute_reply.started": "2023-03-17T19:46:02.144962Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X after trimming: (2115, 22, 250)\n",
      "Shape of X after maxpooling: (2115, 22, 125)\n",
      "Shape of X after averaging+noise and concatenating: (4230, 22, 125)\n",
      "Shape of X after subsampling and concatenating: (8460, 22, 125)\n",
      "Shape of X after trimming: (443, 22, 250)\n",
      "Shape of X after maxpooling: (443, 22, 125)\n",
      "Shape of X after averaging+noise and concatenating: (886, 22, 125)\n",
      "Shape of X after subsampling and concatenating: (1772, 22, 125)\n",
      "shape of train sequence (8460, 125, 22)\n",
      "shape of train sequence label (8460, 4)\n",
      "shape of test sequence (1772, 125, 22)\n",
      "shape of test sequence label (1772, 4)\n",
      "Epoch 1/100\n",
      "133/133 [==============================] - 6s 12ms/step - loss: 1.3751 - accuracy: 0.2935\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 1.3239 - accuracy: 0.3630\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 1.2628 - accuracy: 0.4169\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 1.2268 - accuracy: 0.4508\n",
      "Epoch 5/100\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 1.1845 - accuracy: 0.4755\n",
      "Epoch 6/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 1.1535 - accuracy: 0.4952\n",
      "Epoch 7/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.1218 - accuracy: 0.5194\n",
      "Epoch 8/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 1.0844 - accuracy: 0.5475\n",
      "Epoch 9/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 1.0577 - accuracy: 0.5584\n",
      "Epoch 10/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 1.0302 - accuracy: 0.5734\n",
      "Epoch 11/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 1.0045 - accuracy: 0.5822\n",
      "Epoch 12/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.9873 - accuracy: 0.5894\n",
      "Epoch 13/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.9623 - accuracy: 0.6064\n",
      "Epoch 14/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.9479 - accuracy: 0.6135\n",
      "Epoch 15/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.9180 - accuracy: 0.6300\n",
      "Epoch 16/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.8850 - accuracy: 0.6448\n",
      "Epoch 17/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.8751 - accuracy: 0.6538\n",
      "Epoch 18/100\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.8667 - accuracy: 0.6514\n",
      "Epoch 19/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.8629 - accuracy: 0.6558\n",
      "Epoch 20/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.8380 - accuracy: 0.6695\n",
      "Epoch 21/100\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.8316 - accuracy: 0.6687\n",
      "Epoch 22/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.8223 - accuracy: 0.6699\n",
      "Epoch 23/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.7995 - accuracy: 0.6851\n",
      "Epoch 24/100\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.7912 - accuracy: 0.6864\n",
      "Epoch 25/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.7890 - accuracy: 0.6933\n",
      "Epoch 26/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.7756 - accuracy: 0.6955\n",
      "Epoch 27/100\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.7593 - accuracy: 0.7061\n",
      "Epoch 28/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.7478 - accuracy: 0.7112\n",
      "Epoch 29/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.7428 - accuracy: 0.7099\n",
      "Epoch 30/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.7236 - accuracy: 0.7186\n",
      "Epoch 31/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.7184 - accuracy: 0.7226\n",
      "Epoch 32/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.7106 - accuracy: 0.7239\n",
      "Epoch 33/100\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.7054 - accuracy: 0.7284\n",
      "Epoch 34/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.7114 - accuracy: 0.7208\n",
      "Epoch 35/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.6978 - accuracy: 0.7319\n",
      "Epoch 36/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6978 - accuracy: 0.7275\n",
      "Epoch 37/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6839 - accuracy: 0.7337\n",
      "Epoch 38/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.6778 - accuracy: 0.7400\n",
      "Epoch 39/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.6723 - accuracy: 0.7429\n",
      "Epoch 40/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6669 - accuracy: 0.7440\n",
      "Epoch 41/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.6733 - accuracy: 0.7410\n",
      "Epoch 42/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.6651 - accuracy: 0.7459\n",
      "Epoch 43/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6670 - accuracy: 0.7421\n",
      "Epoch 44/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6555 - accuracy: 0.7460\n",
      "Epoch 45/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6542 - accuracy: 0.7512\n",
      "Epoch 46/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.6457 - accuracy: 0.7483\n",
      "Epoch 47/100\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6379 - accuracy: 0.7582\n",
      "Epoch 48/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6335 - accuracy: 0.7564\n",
      "Epoch 49/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.6400 - accuracy: 0.7537\n",
      "Epoch 50/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.6272 - accuracy: 0.7561\n",
      "Epoch 51/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6294 - accuracy: 0.7566\n",
      "Epoch 52/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6235 - accuracy: 0.7592\n",
      "Epoch 53/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.6001 - accuracy: 0.7707\n",
      "Epoch 54/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.6283 - accuracy: 0.7603\n",
      "Epoch 55/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6185 - accuracy: 0.7667\n",
      "Epoch 56/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6140 - accuracy: 0.7645\n",
      "Epoch 57/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.6176 - accuracy: 0.7625\n",
      "Epoch 58/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5991 - accuracy: 0.7764\n",
      "Epoch 59/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.6010 - accuracy: 0.7689\n",
      "Epoch 60/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.5950 - accuracy: 0.7693\n",
      "Epoch 61/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5897 - accuracy: 0.7730\n",
      "Epoch 62/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5841 - accuracy: 0.7790\n",
      "Epoch 63/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5844 - accuracy: 0.7752\n",
      "Epoch 64/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5675 - accuracy: 0.7822\n",
      "Epoch 65/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5855 - accuracy: 0.7781\n",
      "Epoch 66/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5767 - accuracy: 0.7877\n",
      "Epoch 67/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5833 - accuracy: 0.7746\n",
      "Epoch 68/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5698 - accuracy: 0.7772\n",
      "Epoch 69/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5811 - accuracy: 0.7767\n",
      "Epoch 70/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5657 - accuracy: 0.7856\n",
      "Epoch 71/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5427 - accuracy: 0.7939\n",
      "Epoch 72/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5739 - accuracy: 0.7799\n",
      "Epoch 73/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.5575 - accuracy: 0.7895\n",
      "Epoch 74/100\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.5543 - accuracy: 0.7889\n",
      "Epoch 75/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5596 - accuracy: 0.7928\n",
      "Epoch 76/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.5530 - accuracy: 0.7915\n",
      "Epoch 77/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.5442 - accuracy: 0.7935\n",
      "Epoch 78/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.5465 - accuracy: 0.7928\n",
      "Epoch 79/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5536 - accuracy: 0.7894\n",
      "Epoch 80/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5495 - accuracy: 0.7917\n",
      "Epoch 81/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5384 - accuracy: 0.7987\n",
      "Epoch 82/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5368 - accuracy: 0.7965\n",
      "Epoch 83/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.5419 - accuracy: 0.7921\n",
      "Epoch 84/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.5271 - accuracy: 0.7974\n",
      "Epoch 85/100\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.5479 - accuracy: 0.7924\n",
      "Epoch 86/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.5307 - accuracy: 0.8006\n",
      "Epoch 87/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5277 - accuracy: 0.8025\n",
      "Epoch 88/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5434 - accuracy: 0.7920\n",
      "Epoch 89/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5252 - accuracy: 0.8045\n",
      "Epoch 90/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.5192 - accuracy: 0.8037\n",
      "Epoch 91/100\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.5227 - accuracy: 0.8072\n",
      "Epoch 92/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.5110 - accuracy: 0.8082\n",
      "Epoch 93/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.5142 - accuracy: 0.8096\n",
      "Epoch 94/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.5152 - accuracy: 0.8053\n",
      "Epoch 95/100\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.5113 - accuracy: 0.8059\n",
      "Epoch 96/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5073 - accuracy: 0.8077\n",
      "Epoch 97/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.5065 - accuracy: 0.8074\n",
      "Epoch 98/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.5057 - accuracy: 0.8060\n",
      "Epoch 99/100\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.5111 - accuracy: 0.8077\n",
      "Epoch 100/100\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4904 - accuracy: 0.8161\n",
      "Test accuracy of the CNN_LSTM model: 0.6924379467964172\n",
      "Shape of X after trimming: (2115, 22, 500)\n",
      "Shape of X after maxpooling: (2115, 22, 250)\n",
      "Shape of X after averaging+noise and concatenating: (4230, 22, 250)\n",
      "Shape of X after subsampling and concatenating: (8460, 22, 250)\n",
      "Shape of X after trimming: (443, 22, 500)\n",
      "Shape of X after maxpooling: (443, 22, 250)\n",
      "Shape of X after averaging+noise and concatenating: (886, 22, 250)\n",
      "Shape of X after subsampling and concatenating: (1772, 22, 250)\n",
      "shape of train sequence (8460, 250, 22)\n",
      "shape of train sequence label (8460, 4)\n",
      "shape of test sequence (1772, 250, 22)\n",
      "shape of test sequence label (1772, 4)\n",
      "Epoch 1/100\n",
      "133/133 [==============================] - 7s 13ms/step - loss: 1.3776 - accuracy: 0.2817\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 1.3113 - accuracy: 0.3700\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 1.2421 - accuracy: 0.4206\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 1.1982 - accuracy: 0.4530\n",
      "Epoch 5/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 1.1622 - accuracy: 0.4785\n",
      "Epoch 6/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 1.1309 - accuracy: 0.4969\n",
      "Epoch 7/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 1.1020 - accuracy: 0.5191\n",
      "Epoch 8/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 1.0536 - accuracy: 0.5554\n",
      "Epoch 9/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0091 - accuracy: 0.5818\n",
      "Epoch 10/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.9595 - accuracy: 0.6115\n",
      "Epoch 11/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.9273 - accuracy: 0.6284\n",
      "Epoch 12/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.9036 - accuracy: 0.6395\n",
      "Epoch 13/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.8761 - accuracy: 0.6543\n",
      "Epoch 14/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.8411 - accuracy: 0.6709\n",
      "Epoch 15/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.8234 - accuracy: 0.6791\n",
      "Epoch 16/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.8165 - accuracy: 0.6864\n",
      "Epoch 17/100\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7878 - accuracy: 0.7018\n",
      "Epoch 18/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.7788 - accuracy: 0.6957\n",
      "Epoch 19/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.7495 - accuracy: 0.7050\n",
      "Epoch 20/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.7369 - accuracy: 0.7160\n",
      "Epoch 21/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.7456 - accuracy: 0.7171\n",
      "Epoch 22/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7097 - accuracy: 0.7307\n",
      "Epoch 23/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.7037 - accuracy: 0.7351\n",
      "Epoch 24/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.6880 - accuracy: 0.7409\n",
      "Epoch 25/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6913 - accuracy: 0.7370\n",
      "Epoch 26/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6728 - accuracy: 0.7514\n",
      "Epoch 27/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6534 - accuracy: 0.7520\n",
      "Epoch 28/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6475 - accuracy: 0.7551\n",
      "Epoch 29/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6484 - accuracy: 0.7552\n",
      "Epoch 30/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.6407 - accuracy: 0.7560\n",
      "Epoch 31/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6264 - accuracy: 0.7632\n",
      "Epoch 32/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6168 - accuracy: 0.7764\n",
      "Epoch 33/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.6113 - accuracy: 0.7686\n",
      "Epoch 34/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.6034 - accuracy: 0.7766\n",
      "Epoch 35/100\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.6022 - accuracy: 0.7728\n",
      "Epoch 36/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5847 - accuracy: 0.7784\n",
      "Epoch 37/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5844 - accuracy: 0.7838\n",
      "Epoch 38/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5681 - accuracy: 0.7843\n",
      "Epoch 39/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5915 - accuracy: 0.7833\n",
      "Epoch 40/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5721 - accuracy: 0.7888\n",
      "Epoch 41/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5695 - accuracy: 0.7838\n",
      "Epoch 42/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5573 - accuracy: 0.7959\n",
      "Epoch 43/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5328 - accuracy: 0.8020\n",
      "Epoch 44/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5400 - accuracy: 0.7982\n",
      "Epoch 45/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5452 - accuracy: 0.7967\n",
      "Epoch 46/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5361 - accuracy: 0.8030\n",
      "Epoch 47/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5382 - accuracy: 0.8033\n",
      "Epoch 48/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5274 - accuracy: 0.7975\n",
      "Epoch 49/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5122 - accuracy: 0.8084\n",
      "Epoch 50/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5171 - accuracy: 0.8086\n",
      "Epoch 51/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5228 - accuracy: 0.8063\n",
      "Epoch 52/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5006 - accuracy: 0.8182\n",
      "Epoch 53/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4914 - accuracy: 0.8149\n",
      "Epoch 54/100\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.4913 - accuracy: 0.8199\n",
      "Epoch 55/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4982 - accuracy: 0.8154\n",
      "Epoch 56/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4873 - accuracy: 0.8176\n",
      "Epoch 57/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5024 - accuracy: 0.8152\n",
      "Epoch 58/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4818 - accuracy: 0.8215\n",
      "Epoch 59/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4727 - accuracy: 0.8275\n",
      "Epoch 60/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4577 - accuracy: 0.8339\n",
      "Epoch 61/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4744 - accuracy: 0.8216\n",
      "Epoch 62/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4662 - accuracy: 0.8285\n",
      "Epoch 63/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4508 - accuracy: 0.8327\n",
      "Epoch 64/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4895 - accuracy: 0.8167\n",
      "Epoch 65/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4610 - accuracy: 0.8301\n",
      "Epoch 66/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4494 - accuracy: 0.8382\n",
      "Epoch 67/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4449 - accuracy: 0.8361\n",
      "Epoch 68/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4506 - accuracy: 0.8340\n",
      "Epoch 69/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4399 - accuracy: 0.8353\n",
      "Epoch 70/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4533 - accuracy: 0.8324\n",
      "Epoch 71/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4392 - accuracy: 0.8351\n",
      "Epoch 72/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4370 - accuracy: 0.8377\n",
      "Epoch 73/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4587 - accuracy: 0.8290\n",
      "Epoch 74/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4269 - accuracy: 0.8410\n",
      "Epoch 75/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4307 - accuracy: 0.8395\n",
      "Epoch 76/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4188 - accuracy: 0.8454\n",
      "Epoch 77/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4268 - accuracy: 0.8391\n",
      "Epoch 78/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4171 - accuracy: 0.8479\n",
      "Epoch 79/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4128 - accuracy: 0.8499\n",
      "Epoch 80/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4238 - accuracy: 0.8434\n",
      "Epoch 81/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4287 - accuracy: 0.8468\n",
      "Epoch 82/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4210 - accuracy: 0.8417\n",
      "Epoch 83/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3995 - accuracy: 0.8499\n",
      "Epoch 84/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4044 - accuracy: 0.8496\n",
      "Epoch 85/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4346 - accuracy: 0.8397\n",
      "Epoch 86/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4050 - accuracy: 0.8504\n",
      "Epoch 87/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4051 - accuracy: 0.8554\n",
      "Epoch 88/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4157 - accuracy: 0.8482\n",
      "Epoch 89/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4039 - accuracy: 0.8534\n",
      "Epoch 90/100\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.3902 - accuracy: 0.8583\n",
      "Epoch 91/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4062 - accuracy: 0.8505\n",
      "Epoch 92/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.3891 - accuracy: 0.8584\n",
      "Epoch 93/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3993 - accuracy: 0.8540\n",
      "Epoch 94/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4022 - accuracy: 0.8556\n",
      "Epoch 95/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3824 - accuracy: 0.8590\n",
      "Epoch 96/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4021 - accuracy: 0.8502\n",
      "Epoch 97/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3916 - accuracy: 0.8563\n",
      "Epoch 98/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.3857 - accuracy: 0.8584\n",
      "Epoch 99/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3724 - accuracy: 0.8618\n",
      "Epoch 100/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3870 - accuracy: 0.8579\n",
      "Test accuracy of the CNN_LSTM model: 0.7370203137397766\n",
      "Shape of X after trimming: (2115, 22, 750)\n",
      "Shape of X after maxpooling: (2115, 22, 375)\n",
      "Shape of X after averaging+noise and concatenating: (4230, 22, 375)\n",
      "Shape of X after subsampling and concatenating: (8460, 22, 375)\n",
      "Shape of X after trimming: (443, 22, 750)\n",
      "Shape of X after maxpooling: (443, 22, 375)\n",
      "Shape of X after averaging+noise and concatenating: (886, 22, 375)\n",
      "Shape of X after subsampling and concatenating: (1772, 22, 375)\n",
      "shape of train sequence (8460, 375, 22)\n",
      "shape of train sequence label (8460, 4)\n",
      "shape of test sequence (1772, 375, 22)\n",
      "shape of test sequence label (1772, 4)\n",
      "Epoch 1/100\n",
      "133/133 [==============================] - 7s 16ms/step - loss: 1.3829 - accuracy: 0.2779\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.3337 - accuracy: 0.3420\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.2581 - accuracy: 0.4037\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.2080 - accuracy: 0.4479\n",
      "Epoch 5/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.1731 - accuracy: 0.4715\n",
      "Epoch 6/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 1.1332 - accuracy: 0.4930\n",
      "Epoch 7/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0983 - accuracy: 0.5246\n",
      "Epoch 8/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0444 - accuracy: 0.5621\n",
      "Epoch 9/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 1.0066 - accuracy: 0.5836\n",
      "Epoch 10/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.9773 - accuracy: 0.6005\n",
      "Epoch 11/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9336 - accuracy: 0.6254\n",
      "Epoch 12/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.9039 - accuracy: 0.6439\n",
      "Epoch 13/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8820 - accuracy: 0.6548\n",
      "Epoch 14/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8663 - accuracy: 0.6553\n",
      "Epoch 15/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8314 - accuracy: 0.6748\n",
      "Epoch 16/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8106 - accuracy: 0.6824\n",
      "Epoch 17/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7888 - accuracy: 0.7000\n",
      "Epoch 18/100\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7750 - accuracy: 0.6991\n",
      "Epoch 19/100\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.7714 - accuracy: 0.6993\n",
      "Epoch 20/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.7412 - accuracy: 0.7115\n",
      "Epoch 21/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.7291 - accuracy: 0.7230\n",
      "Epoch 22/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.7324 - accuracy: 0.7251\n",
      "Epoch 23/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.6979 - accuracy: 0.7357\n",
      "Epoch 24/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.6984 - accuracy: 0.7297\n",
      "Epoch 25/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.6773 - accuracy: 0.7452\n",
      "Epoch 26/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6745 - accuracy: 0.7481\n",
      "Epoch 27/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.6601 - accuracy: 0.7487\n",
      "Epoch 28/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.6530 - accuracy: 0.7532\n",
      "Epoch 29/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6287 - accuracy: 0.7621\n",
      "Epoch 30/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.6149 - accuracy: 0.7636\n",
      "Epoch 31/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6114 - accuracy: 0.7703\n",
      "Epoch 32/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6176 - accuracy: 0.7694\n",
      "Epoch 33/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.6097 - accuracy: 0.7689\n",
      "Epoch 34/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5957 - accuracy: 0.7751\n",
      "Epoch 35/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5856 - accuracy: 0.7824\n",
      "Epoch 36/100\n",
      "133/133 [==============================] - 3s 20ms/step - loss: 0.5898 - accuracy: 0.7787\n",
      "Epoch 37/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5618 - accuracy: 0.7927\n",
      "Epoch 38/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5705 - accuracy: 0.7885\n",
      "Epoch 39/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5563 - accuracy: 0.7928\n",
      "Epoch 40/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5495 - accuracy: 0.7975\n",
      "Epoch 41/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5571 - accuracy: 0.7896\n",
      "Epoch 42/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.5452 - accuracy: 0.7961\n",
      "Epoch 43/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5332 - accuracy: 0.8056\n",
      "Epoch 44/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5284 - accuracy: 0.8035\n",
      "Epoch 45/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5336 - accuracy: 0.8015\n",
      "Epoch 46/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5350 - accuracy: 0.8022\n",
      "Epoch 47/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5192 - accuracy: 0.8078\n",
      "Epoch 48/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.5203 - accuracy: 0.8066\n",
      "Epoch 49/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5110 - accuracy: 0.8124\n",
      "Epoch 50/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4977 - accuracy: 0.8119\n",
      "Epoch 51/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4828 - accuracy: 0.8258\n",
      "Epoch 52/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4911 - accuracy: 0.8184\n",
      "Epoch 53/100\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.4847 - accuracy: 0.8196\n",
      "Epoch 54/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4946 - accuracy: 0.8135\n",
      "Epoch 55/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4717 - accuracy: 0.8212\n",
      "Epoch 56/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4739 - accuracy: 0.8249\n",
      "Epoch 57/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4766 - accuracy: 0.8207\n",
      "Epoch 58/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4718 - accuracy: 0.8247\n",
      "Epoch 59/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4644 - accuracy: 0.8261\n",
      "Epoch 60/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4603 - accuracy: 0.8272\n",
      "Epoch 61/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4549 - accuracy: 0.8336\n",
      "Epoch 62/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4518 - accuracy: 0.8338\n",
      "Epoch 63/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4643 - accuracy: 0.8277\n",
      "Epoch 64/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4244 - accuracy: 0.8472\n",
      "Epoch 65/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4520 - accuracy: 0.8326\n",
      "Epoch 66/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4484 - accuracy: 0.8304\n",
      "Epoch 67/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4505 - accuracy: 0.8337\n",
      "Epoch 68/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4314 - accuracy: 0.8410\n",
      "Epoch 69/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4293 - accuracy: 0.8415\n",
      "Epoch 70/100\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.4413 - accuracy: 0.8383\n",
      "Epoch 71/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4306 - accuracy: 0.8402\n",
      "Epoch 72/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4127 - accuracy: 0.8441\n",
      "Epoch 73/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4271 - accuracy: 0.8440\n",
      "Epoch 74/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4144 - accuracy: 0.8491\n",
      "Epoch 75/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4045 - accuracy: 0.8543\n",
      "Epoch 76/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4115 - accuracy: 0.8534\n",
      "Epoch 77/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4062 - accuracy: 0.8519\n",
      "Epoch 78/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4044 - accuracy: 0.8467\n",
      "Epoch 79/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4108 - accuracy: 0.8514\n",
      "Epoch 80/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3986 - accuracy: 0.8564\n",
      "Epoch 81/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.3838 - accuracy: 0.8560\n",
      "Epoch 82/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3891 - accuracy: 0.8557\n",
      "Epoch 83/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.3925 - accuracy: 0.8570\n",
      "Epoch 84/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.3857 - accuracy: 0.8569\n",
      "Epoch 85/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3847 - accuracy: 0.8556\n",
      "Epoch 86/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3918 - accuracy: 0.8569\n",
      "Epoch 87/100\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.3791 - accuracy: 0.8618\n",
      "Epoch 88/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3835 - accuracy: 0.8598\n",
      "Epoch 89/100\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.3792 - accuracy: 0.8616\n",
      "Epoch 90/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3723 - accuracy: 0.8675\n",
      "Epoch 91/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3657 - accuracy: 0.8641\n",
      "Epoch 92/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.3761 - accuracy: 0.8623\n",
      "Epoch 93/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3672 - accuracy: 0.8631\n",
      "Epoch 94/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.3639 - accuracy: 0.8639\n",
      "Epoch 95/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.3751 - accuracy: 0.8630\n",
      "Epoch 96/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3738 - accuracy: 0.8609\n",
      "Epoch 97/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3693 - accuracy: 0.8671\n",
      "Epoch 98/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3683 - accuracy: 0.8636\n",
      "Epoch 99/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.3558 - accuracy: 0.8667\n",
      "Epoch 100/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.3640 - accuracy: 0.8634\n",
      "Test accuracy of the CNN_LSTM model: 0.715575635433197\n",
      "Shape of X after trimming: (2115, 22, 1000)\n",
      "Shape of X after maxpooling: (2115, 22, 500)\n",
      "Shape of X after averaging+noise and concatenating: (4230, 22, 500)\n",
      "Shape of X after subsampling and concatenating: (8460, 22, 500)\n",
      "Shape of X after trimming: (443, 22, 1000)\n",
      "Shape of X after maxpooling: (443, 22, 500)\n",
      "Shape of X after averaging+noise and concatenating: (886, 22, 500)\n",
      "Shape of X after subsampling and concatenating: (1772, 22, 500)\n",
      "shape of train sequence (8460, 500, 22)\n",
      "shape of train sequence label (8460, 4)\n",
      "shape of test sequence (1772, 500, 22)\n",
      "shape of test sequence label (1772, 4)\n",
      "Epoch 1/100\n",
      "133/133 [==============================] - 8s 17ms/step - loss: 1.3903 - accuracy: 0.2709\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 1.3649 - accuracy: 0.2999\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 1.3028 - accuracy: 0.3664\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 1.2447 - accuracy: 0.4209\n",
      "Epoch 5/100\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 1.1923 - accuracy: 0.4600\n",
      "Epoch 6/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 1.1631 - accuracy: 0.4790\n",
      "Epoch 7/100\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.1265 - accuracy: 0.5040\n",
      "Epoch 8/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 1.0982 - accuracy: 0.5152\n",
      "Epoch 9/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 1.0729 - accuracy: 0.5352\n",
      "Epoch 10/100\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.0480 - accuracy: 0.5498\n",
      "Epoch 11/100\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 1.0033 - accuracy: 0.5752\n",
      "Epoch 12/100\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.9829 - accuracy: 0.5959\n",
      "Epoch 13/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.9457 - accuracy: 0.6149\n",
      "Epoch 14/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.9143 - accuracy: 0.6339\n",
      "Epoch 15/100\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.8758 - accuracy: 0.6521\n",
      "Epoch 16/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.8377 - accuracy: 0.6696\n",
      "Epoch 17/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.8217 - accuracy: 0.6798\n",
      "Epoch 18/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.8019 - accuracy: 0.6888\n",
      "Epoch 19/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.7774 - accuracy: 0.6937\n",
      "Epoch 20/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.7569 - accuracy: 0.7077\n",
      "Epoch 21/100\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7298 - accuracy: 0.7203\n",
      "Epoch 22/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.7152 - accuracy: 0.7287\n",
      "Epoch 23/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.7077 - accuracy: 0.7281\n",
      "Epoch 24/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.6940 - accuracy: 0.7355\n",
      "Epoch 25/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.6908 - accuracy: 0.7344\n",
      "Epoch 26/100\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.6644 - accuracy: 0.7474\n",
      "Epoch 27/100\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.6646 - accuracy: 0.7462\n",
      "Epoch 28/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.6378 - accuracy: 0.7569\n",
      "Epoch 29/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.6424 - accuracy: 0.7548\n",
      "Epoch 30/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.6181 - accuracy: 0.7661\n",
      "Epoch 31/100\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6186 - accuracy: 0.7657\n",
      "Epoch 32/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.6081 - accuracy: 0.7677\n",
      "Epoch 33/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.5867 - accuracy: 0.7773\n",
      "Epoch 34/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.6025 - accuracy: 0.7740\n",
      "Epoch 35/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.5948 - accuracy: 0.7753\n",
      "Epoch 36/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.5807 - accuracy: 0.7793\n",
      "Epoch 37/100\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.5621 - accuracy: 0.7891\n",
      "Epoch 38/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.5496 - accuracy: 0.7955\n",
      "Epoch 39/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.5530 - accuracy: 0.7927\n",
      "Epoch 40/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.5474 - accuracy: 0.7918\n",
      "Epoch 41/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.5519 - accuracy: 0.7889\n",
      "Epoch 42/100\n",
      "133/133 [==============================] - 3s 21ms/step - loss: 0.5351 - accuracy: 0.7980\n",
      "Epoch 43/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.5184 - accuracy: 0.8030\n",
      "Epoch 44/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.5100 - accuracy: 0.8090\n",
      "Epoch 45/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.5213 - accuracy: 0.8020\n",
      "Epoch 46/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.5164 - accuracy: 0.8058\n",
      "Epoch 47/100\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.5001 - accuracy: 0.8130\n",
      "Epoch 48/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.5018 - accuracy: 0.8099\n",
      "Epoch 49/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.5020 - accuracy: 0.8148\n",
      "Epoch 50/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4858 - accuracy: 0.8145\n",
      "Epoch 51/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4930 - accuracy: 0.8132\n",
      "Epoch 52/100\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5046 - accuracy: 0.8093\n",
      "Epoch 53/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4660 - accuracy: 0.8294\n",
      "Epoch 54/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4676 - accuracy: 0.8274\n",
      "Epoch 55/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4675 - accuracy: 0.8248\n",
      "Epoch 56/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4625 - accuracy: 0.8233\n",
      "Epoch 57/100\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 0.4578 - accuracy: 0.8312\n",
      "Epoch 58/100\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.4386 - accuracy: 0.8366\n",
      "Epoch 59/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4489 - accuracy: 0.8322\n",
      "Epoch 60/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4425 - accuracy: 0.8384\n",
      "Epoch 61/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4486 - accuracy: 0.8314\n",
      "Epoch 62/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4539 - accuracy: 0.8307\n",
      "Epoch 63/100\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.4359 - accuracy: 0.8376\n",
      "Epoch 64/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4341 - accuracy: 0.8385\n",
      "Epoch 65/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4338 - accuracy: 0.8401\n",
      "Epoch 66/100\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.4240 - accuracy: 0.8409\n",
      "Epoch 67/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4285 - accuracy: 0.8408\n",
      "Epoch 68/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4146 - accuracy: 0.8455\n",
      "Epoch 69/100\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.4267 - accuracy: 0.8440\n",
      "Epoch 70/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4134 - accuracy: 0.8457\n",
      "Epoch 71/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4310 - accuracy: 0.8355\n",
      "Epoch 72/100\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.4168 - accuracy: 0.8461\n",
      "Epoch 73/100\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.4036 - accuracy: 0.8509\n",
      "Epoch 74/100\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.4015 - accuracy: 0.8540\n",
      "Epoch 75/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.3852 - accuracy: 0.8574\n",
      "Epoch 76/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4030 - accuracy: 0.8495\n",
      "Epoch 77/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.3964 - accuracy: 0.8556\n",
      "Epoch 78/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.3900 - accuracy: 0.8578\n",
      "Epoch 79/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.3808 - accuracy: 0.8583\n",
      "Epoch 80/100\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.3942 - accuracy: 0.8541\n",
      "Epoch 81/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.3978 - accuracy: 0.8533\n",
      "Epoch 82/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.3750 - accuracy: 0.8611\n",
      "Epoch 83/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.3675 - accuracy: 0.8592\n",
      "Epoch 84/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.3717 - accuracy: 0.8650\n",
      "Epoch 85/100\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.3923 - accuracy: 0.8539\n",
      "Epoch 86/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.3858 - accuracy: 0.8557\n",
      "Epoch 87/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.3697 - accuracy: 0.8630\n",
      "Epoch 88/100\n",
      "133/133 [==============================] - 2s 18ms/step - loss: 0.3647 - accuracy: 0.8669\n",
      "Epoch 89/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.3606 - accuracy: 0.8680\n",
      "Epoch 90/100\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.3688 - accuracy: 0.8661\n",
      "Epoch 91/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.3510 - accuracy: 0.8693\n",
      "Epoch 92/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.3504 - accuracy: 0.8684\n",
      "Epoch 93/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.3515 - accuracy: 0.8714\n",
      "Epoch 94/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.3492 - accuracy: 0.8683\n",
      "Epoch 95/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.3644 - accuracy: 0.8651\n",
      "Epoch 96/100\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.3345 - accuracy: 0.8735\n",
      "Epoch 97/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.3470 - accuracy: 0.8730\n",
      "Epoch 98/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.3506 - accuracy: 0.8706\n",
      "Epoch 99/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.3536 - accuracy: 0.8654\n",
      "Epoch 100/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.3304 - accuracy: 0.8798\n",
      "Test accuracy of the CNN_LSTM model: 0.7206546068191528\n"
     ]
    }
   ],
   "source": [
    "#CNN_LSTM\n",
    "time_sequence=[250,500,750,1000]\n",
    "CNN_LSTM_time_accuracy1=[]\n",
    "for trim_time in time_sequence:\n",
    "    X_train_sequence,y_train_sequence=process_data(X_train_valid,y_train_valid,trim_time=trim_time)\n",
    "    X_test_sequence,y_test_sequence=process_data(X_test,y_test_original,trim_time=trim_time)\n",
    "    X_train_sequence = X_train_sequence.reshape((X_train_sequence.shape[0], X_train_sequence.shape[1], X_train_sequence.shape[3]))\n",
    "    X_test_sequence = X_test_sequence.reshape((X_test_sequence.shape[0], X_test_sequence.shape[1], X_test_sequence.shape[3]))\n",
    "    print('shape of train sequence',np.shape(X_train_sequence))\n",
    "    print('shape of train sequence label',np.shape(y_train_sequence))\n",
    "    print('shape of test sequence',np.shape(X_test_sequence))\n",
    "    print('shape of test sequence label',np.shape(y_test_sequence))\n",
    "    learning_rate = 1e-3\n",
    "    epochs=100\n",
    "    cnn_optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    hybrid_CNN_LSTM_model=CNN_LSTM(trim_time=trim_time)\n",
    "    hybrid_CNN_LSTM_model.compile(loss='categorical_crossentropy',\n",
    "                 optimizer=cnn_optimizer,\n",
    "                 metrics=['accuracy'])\n",
    "    hybrid_CNN_LSTM_results =  hybrid_CNN_LSTM_model.fit(X_train_sequence,y_train_sequence,\n",
    "             batch_size=64,\n",
    "             epochs=epochs,verbose=True)\n",
    "    cnn_lstm_score= hybrid_CNN_LSTM_model.evaluate(X_test_sequence,y_test_sequence,verbose=0)\n",
    "    CNN_LSTM_time_accuracy1.append(cnn_lstm_score[1])\n",
    "    print('Test accuracy of the CNN_LSTM model:',cnn_lstm_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-kkp6xxQ4Iqo"
   },
   "source": [
    "## Investigate time points between 250 and 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-17T19:06:40.140333Z",
     "iopub.status.busy": "2023-03-17T19:06:40.139605Z",
     "iopub.status.idle": "2023-03-17T19:15:35.195734Z",
     "shell.execute_reply": "2023-03-17T19:15:35.194586Z",
     "shell.execute_reply.started": "2023-03-17T19:06:40.140294Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X after trimming: (2115, 22, 300)\n",
      "Shape of X after maxpooling: (2115, 22, 150)\n",
      "Shape of X after averaging+noise and concatenating: (4230, 22, 150)\n",
      "Shape of X after subsampling and concatenating: (8460, 22, 150)\n",
      "Shape of X after trimming: (443, 22, 300)\n",
      "Shape of X after maxpooling: (443, 22, 150)\n",
      "Shape of X after averaging+noise and concatenating: (886, 22, 150)\n",
      "Shape of X after subsampling and concatenating: (1772, 22, 150)\n",
      "shape of train sequence (8460, 150, 1, 22)\n",
      "shape of train sequence label (8460, 4)\n",
      "shape of test sequence (1772, 150, 1, 22)\n",
      "shape of test sequence label (1772, 4)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 150, 1, 10)        2210      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 50, 1, 10)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 50, 1, 10)        40        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 50, 1, 10)         0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 50, 1, 25)         2525      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 17, 1, 25)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 17, 1, 25)        100       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 17, 1, 25)         0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 17, 1, 50)         12550     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 6, 1, 50)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 6, 1, 50)         200       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 6, 1, 50)          0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 6, 1, 100)         50100     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 2, 1, 100)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 2, 1, 100)        400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 2, 1, 100)         0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 2, 1, 200)         200200    \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 1, 1, 200)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 1, 1, 200)        800       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 1, 1, 200)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 200)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 804       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 269,929\n",
      "Trainable params: 269,159\n",
      "Non-trainable params: 770\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-17 19:06:49.096106: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential/dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 14s 7ms/step - loss: 2.0226 - accuracy: 0.2739\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.6445 - accuracy: 0.2810\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.4992 - accuracy: 0.3004\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 1.4292 - accuracy: 0.3038\n",
      "Epoch 5/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.3844 - accuracy: 0.3294\n",
      "Epoch 6/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.3371 - accuracy: 0.3558\n",
      "Epoch 7/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.2958 - accuracy: 0.3891\n",
      "Epoch 8/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.2710 - accuracy: 0.4157\n",
      "Epoch 9/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.2477 - accuracy: 0.4267\n",
      "Epoch 10/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.2264 - accuracy: 0.4488\n",
      "Epoch 11/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.1998 - accuracy: 0.4667\n",
      "Epoch 12/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.1833 - accuracy: 0.4785\n",
      "Epoch 13/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.1491 - accuracy: 0.5065\n",
      "Epoch 14/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.1426 - accuracy: 0.5067\n",
      "Epoch 15/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.1231 - accuracy: 0.5178\n",
      "Epoch 16/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.0990 - accuracy: 0.5311\n",
      "Epoch 17/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 1.0736 - accuracy: 0.5421\n",
      "Epoch 18/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.0587 - accuracy: 0.5541\n",
      "Epoch 19/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.0467 - accuracy: 0.5585\n",
      "Epoch 20/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.0291 - accuracy: 0.5674\n",
      "Epoch 21/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.0095 - accuracy: 0.5762\n",
      "Epoch 22/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.9980 - accuracy: 0.5905\n",
      "Epoch 23/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.9933 - accuracy: 0.5859\n",
      "Epoch 24/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.9774 - accuracy: 0.5973\n",
      "Epoch 25/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.9638 - accuracy: 0.6082\n",
      "Epoch 26/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.9491 - accuracy: 0.6135\n",
      "Epoch 27/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.9409 - accuracy: 0.6124\n",
      "Epoch 28/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.9350 - accuracy: 0.6119\n",
      "Epoch 29/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.9354 - accuracy: 0.6108\n",
      "Epoch 30/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.9193 - accuracy: 0.6245\n",
      "Epoch 31/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.9152 - accuracy: 0.6239\n",
      "Epoch 32/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.9066 - accuracy: 0.6309\n",
      "Epoch 33/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.8907 - accuracy: 0.6318\n",
      "Epoch 34/100\n",
      "133/133 [==============================] - 1s 9ms/step - loss: 0.8857 - accuracy: 0.6355\n",
      "Epoch 35/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8734 - accuracy: 0.6443\n",
      "Epoch 36/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8651 - accuracy: 0.6473\n",
      "Epoch 37/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8720 - accuracy: 0.6474\n",
      "Epoch 38/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8649 - accuracy: 0.6480\n",
      "Epoch 39/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8618 - accuracy: 0.6502\n",
      "Epoch 40/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8429 - accuracy: 0.6617\n",
      "Epoch 41/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8515 - accuracy: 0.6545\n",
      "Epoch 42/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8444 - accuracy: 0.6591\n",
      "Epoch 43/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.8301 - accuracy: 0.6619\n",
      "Epoch 44/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8375 - accuracy: 0.6677\n",
      "Epoch 45/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8225 - accuracy: 0.6699\n",
      "Epoch 46/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8319 - accuracy: 0.6651\n",
      "Epoch 47/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8142 - accuracy: 0.6726\n",
      "Epoch 48/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8276 - accuracy: 0.6642\n",
      "Epoch 49/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8087 - accuracy: 0.6743\n",
      "Epoch 50/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7994 - accuracy: 0.6775\n",
      "Epoch 51/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8117 - accuracy: 0.6762\n",
      "Epoch 52/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8083 - accuracy: 0.6729\n",
      "Epoch 53/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8006 - accuracy: 0.6838\n",
      "Epoch 54/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7931 - accuracy: 0.6845\n",
      "Epoch 55/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7861 - accuracy: 0.6869\n",
      "Epoch 56/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.7795 - accuracy: 0.6887\n",
      "Epoch 57/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.7838 - accuracy: 0.6931\n",
      "Epoch 58/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7990 - accuracy: 0.6797\n",
      "Epoch 59/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7823 - accuracy: 0.6858\n",
      "Epoch 60/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7741 - accuracy: 0.6901\n",
      "Epoch 61/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7706 - accuracy: 0.6942\n",
      "Epoch 62/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7733 - accuracy: 0.6986\n",
      "Epoch 63/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7796 - accuracy: 0.6902\n",
      "Epoch 64/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7771 - accuracy: 0.6914\n",
      "Epoch 65/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7735 - accuracy: 0.6865\n",
      "Epoch 66/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7633 - accuracy: 0.6992\n",
      "Epoch 67/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7698 - accuracy: 0.6941\n",
      "Epoch 68/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7598 - accuracy: 0.6962\n",
      "Epoch 69/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7519 - accuracy: 0.7014\n",
      "Epoch 70/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.7583 - accuracy: 0.6926\n",
      "Epoch 71/100\n",
      "133/133 [==============================] - 1s 9ms/step - loss: 0.7496 - accuracy: 0.7060\n",
      "Epoch 72/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.7453 - accuracy: 0.7035\n",
      "Epoch 73/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7433 - accuracy: 0.7007\n",
      "Epoch 74/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7453 - accuracy: 0.7017\n",
      "Epoch 75/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7468 - accuracy: 0.7021\n",
      "Epoch 76/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7237 - accuracy: 0.7202\n",
      "Epoch 77/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7495 - accuracy: 0.7053\n",
      "Epoch 78/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7550 - accuracy: 0.7061\n",
      "Epoch 79/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7301 - accuracy: 0.7138\n",
      "Epoch 80/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7387 - accuracy: 0.7060\n",
      "Epoch 81/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7247 - accuracy: 0.7152\n",
      "Epoch 82/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.7257 - accuracy: 0.7100\n",
      "Epoch 83/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.7326 - accuracy: 0.7108\n",
      "Epoch 84/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7364 - accuracy: 0.7125\n",
      "Epoch 85/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7319 - accuracy: 0.7102\n",
      "Epoch 86/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7340 - accuracy: 0.7071\n",
      "Epoch 87/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7253 - accuracy: 0.7150\n",
      "Epoch 88/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7250 - accuracy: 0.7182\n",
      "Epoch 89/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7208 - accuracy: 0.7187\n",
      "Epoch 90/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7318 - accuracy: 0.7073\n",
      "Epoch 91/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7211 - accuracy: 0.7151\n",
      "Epoch 92/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7138 - accuracy: 0.7201\n",
      "Epoch 93/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7152 - accuracy: 0.7118\n",
      "Epoch 94/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7205 - accuracy: 0.7134\n",
      "Epoch 95/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7094 - accuracy: 0.7177\n",
      "Epoch 96/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.7129 - accuracy: 0.7229\n",
      "Epoch 97/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7163 - accuracy: 0.7118\n",
      "Epoch 98/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7075 - accuracy: 0.7213\n",
      "Epoch 99/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7009 - accuracy: 0.7267\n",
      "Epoch 100/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7103 - accuracy: 0.7181\n",
      "Test accuracy of the basic CNN model: 0.6997742652893066\n",
      "Shape of X after trimming: (2115, 22, 350)\n",
      "Shape of X after maxpooling: (2115, 22, 175)\n",
      "Shape of X after averaging+noise and concatenating: (4230, 22, 175)\n",
      "Shape of X after subsampling and concatenating: (8460, 22, 175)\n",
      "Shape of X after trimming: (443, 22, 350)\n",
      "Shape of X after maxpooling: (443, 22, 175)\n",
      "Shape of X after averaging+noise and concatenating: (886, 22, 175)\n",
      "Shape of X after subsampling and concatenating: (1772, 22, 175)\n",
      "shape of train sequence (8460, 175, 1, 22)\n",
      "shape of train sequence label (8460, 4)\n",
      "shape of test sequence (1772, 175, 1, 22)\n",
      "shape of test sequence label (1772, 4)\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_5 (Conv2D)           (None, 175, 1, 10)        2210      \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 59, 1, 10)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 59, 1, 10)        40        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 59, 1, 10)         0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 59, 1, 25)         2525      \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 20, 1, 25)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 20, 1, 25)        100       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 20, 1, 25)         0         \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 20, 1, 50)         12550     \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 7, 1, 50)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 7, 1, 50)         200       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 7, 1, 50)          0         \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 7, 1, 100)         50100     \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 3, 1, 100)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 3, 1, 100)        400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 3, 1, 100)         0         \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 3, 1, 200)         200200    \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPooling  (None, 1, 1, 200)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 1, 1, 200)        800       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 1, 1, 200)         0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 200)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 804       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 269,929\n",
      "Trainable params: 269,159\n",
      "Non-trainable params: 770\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-17 19:09:14.943520: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_1/dropout_5/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 2s 8ms/step - loss: 2.0117 - accuracy: 0.2726\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.6042 - accuracy: 0.2978\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.4748 - accuracy: 0.3209\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.3778 - accuracy: 0.3574\n",
      "Epoch 5/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.3124 - accuracy: 0.3837\n",
      "Epoch 6/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.2802 - accuracy: 0.4000\n",
      "Epoch 7/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.2522 - accuracy: 0.4215\n",
      "Epoch 8/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.2331 - accuracy: 0.4362\n",
      "Epoch 9/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.2175 - accuracy: 0.4424\n",
      "Epoch 10/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.2070 - accuracy: 0.4423\n",
      "Epoch 11/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.1950 - accuracy: 0.4518\n",
      "Epoch 12/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.1758 - accuracy: 0.4688\n",
      "Epoch 13/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.1545 - accuracy: 0.4923\n",
      "Epoch 14/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 1.1225 - accuracy: 0.5210\n",
      "Epoch 15/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.1032 - accuracy: 0.5298\n",
      "Epoch 16/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.0860 - accuracy: 0.5391\n",
      "Epoch 17/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.0536 - accuracy: 0.5603\n",
      "Epoch 18/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.0401 - accuracy: 0.5727\n",
      "Epoch 19/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.0295 - accuracy: 0.5671\n",
      "Epoch 20/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.0137 - accuracy: 0.5772\n",
      "Epoch 21/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.9896 - accuracy: 0.5874\n",
      "Epoch 22/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.9589 - accuracy: 0.6009\n",
      "Epoch 23/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.9663 - accuracy: 0.6056\n",
      "Epoch 24/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.9490 - accuracy: 0.6116\n",
      "Epoch 25/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.9261 - accuracy: 0.6197\n",
      "Epoch 26/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.9264 - accuracy: 0.6235\n",
      "Epoch 27/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.9162 - accuracy: 0.6294\n",
      "Epoch 28/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.9046 - accuracy: 0.6254\n",
      "Epoch 29/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.8951 - accuracy: 0.6394\n",
      "Epoch 30/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8897 - accuracy: 0.6345\n",
      "Epoch 31/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8798 - accuracy: 0.6440\n",
      "Epoch 32/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8758 - accuracy: 0.6397\n",
      "Epoch 33/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8655 - accuracy: 0.6463\n",
      "Epoch 34/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8534 - accuracy: 0.6526\n",
      "Epoch 35/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8472 - accuracy: 0.6567\n",
      "Epoch 36/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8382 - accuracy: 0.6580\n",
      "Epoch 37/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8306 - accuracy: 0.6618\n",
      "Epoch 38/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8328 - accuracy: 0.6637\n",
      "Epoch 39/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.8237 - accuracy: 0.6695\n",
      "Epoch 40/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.8153 - accuracy: 0.6726\n",
      "Epoch 41/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8210 - accuracy: 0.6639\n",
      "Epoch 42/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7968 - accuracy: 0.6788\n",
      "Epoch 43/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7954 - accuracy: 0.6764\n",
      "Epoch 44/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7983 - accuracy: 0.6772\n",
      "Epoch 45/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7811 - accuracy: 0.6901\n",
      "Epoch 46/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7883 - accuracy: 0.6836\n",
      "Epoch 47/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7902 - accuracy: 0.6830\n",
      "Epoch 48/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7812 - accuracy: 0.6868\n",
      "Epoch 49/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7763 - accuracy: 0.6909\n",
      "Epoch 50/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7625 - accuracy: 0.6908\n",
      "Epoch 51/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7687 - accuracy: 0.6942\n",
      "Epoch 52/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.7649 - accuracy: 0.6939\n",
      "Epoch 53/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.7603 - accuracy: 0.7001\n",
      "Epoch 54/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7425 - accuracy: 0.7050\n",
      "Epoch 55/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7500 - accuracy: 0.7017\n",
      "Epoch 56/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7361 - accuracy: 0.7051\n",
      "Epoch 57/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7477 - accuracy: 0.7035\n",
      "Epoch 58/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.7374 - accuracy: 0.7046\n",
      "Epoch 59/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.7311 - accuracy: 0.7069\n",
      "Epoch 60/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7294 - accuracy: 0.7086\n",
      "Epoch 61/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7367 - accuracy: 0.7113\n",
      "Epoch 62/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7340 - accuracy: 0.7069\n",
      "Epoch 63/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7272 - accuracy: 0.7121\n",
      "Epoch 64/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7222 - accuracy: 0.7180\n",
      "Epoch 65/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.7376 - accuracy: 0.7097\n",
      "Epoch 66/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.7146 - accuracy: 0.7196\n",
      "Epoch 67/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7261 - accuracy: 0.7087\n",
      "Epoch 68/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7085 - accuracy: 0.7125\n",
      "Epoch 69/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7048 - accuracy: 0.7227\n",
      "Epoch 70/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7071 - accuracy: 0.7186\n",
      "Epoch 71/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7030 - accuracy: 0.7187\n",
      "Epoch 72/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7205 - accuracy: 0.7145\n",
      "Epoch 73/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7121 - accuracy: 0.7170\n",
      "Epoch 74/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7063 - accuracy: 0.7210\n",
      "Epoch 75/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6913 - accuracy: 0.7264\n",
      "Epoch 76/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6949 - accuracy: 0.7242\n",
      "Epoch 77/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6867 - accuracy: 0.7267\n",
      "Epoch 78/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.6961 - accuracy: 0.7257\n",
      "Epoch 79/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.6964 - accuracy: 0.7247\n",
      "Epoch 80/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6937 - accuracy: 0.7243\n",
      "Epoch 81/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6969 - accuracy: 0.7241\n",
      "Epoch 82/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6851 - accuracy: 0.7304\n",
      "Epoch 83/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6669 - accuracy: 0.7397\n",
      "Epoch 84/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6725 - accuracy: 0.7309\n",
      "Epoch 85/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6731 - accuracy: 0.7357\n",
      "Epoch 86/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6854 - accuracy: 0.7268\n",
      "Epoch 87/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6746 - accuracy: 0.7329\n",
      "Epoch 88/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6754 - accuracy: 0.7338\n",
      "Epoch 89/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6807 - accuracy: 0.7297\n",
      "Epoch 90/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.6648 - accuracy: 0.7390\n",
      "Epoch 91/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.6656 - accuracy: 0.7384\n",
      "Epoch 92/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.6646 - accuracy: 0.7436\n",
      "Epoch 93/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6629 - accuracy: 0.7415\n",
      "Epoch 94/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6611 - accuracy: 0.7416\n",
      "Epoch 95/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.6572 - accuracy: 0.7424\n",
      "Epoch 96/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.6646 - accuracy: 0.7372\n",
      "Epoch 97/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6628 - accuracy: 0.7372\n",
      "Epoch 98/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6569 - accuracy: 0.7418\n",
      "Epoch 99/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6522 - accuracy: 0.7436\n",
      "Epoch 100/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6558 - accuracy: 0.7427\n",
      "Test accuracy of the basic CNN model: 0.7054176330566406\n",
      "Shape of X after trimming: (2115, 22, 400)\n",
      "Shape of X after maxpooling: (2115, 22, 200)\n",
      "Shape of X after averaging+noise and concatenating: (4230, 22, 200)\n",
      "Shape of X after subsampling and concatenating: (8460, 22, 200)\n",
      "Shape of X after trimming: (443, 22, 400)\n",
      "Shape of X after maxpooling: (443, 22, 200)\n",
      "Shape of X after averaging+noise and concatenating: (886, 22, 200)\n",
      "Shape of X after subsampling and concatenating: (1772, 22, 200)\n",
      "shape of train sequence (8460, 200, 1, 22)\n",
      "shape of train sequence label (8460, 4)\n",
      "shape of test sequence (1772, 200, 1, 22)\n",
      "shape of test sequence label (1772, 4)\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_10 (Conv2D)          (None, 200, 1, 10)        2210      \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPoolin  (None, 67, 1, 10)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 67, 1, 10)        40        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 67, 1, 10)         0         \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 67, 1, 25)         2525      \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPoolin  (None, 23, 1, 25)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 23, 1, 25)        100       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 23, 1, 25)         0         \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 23, 1, 50)         12550     \n",
      "                                                                 \n",
      " max_pooling2d_12 (MaxPoolin  (None, 8, 1, 50)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_12 (Bat  (None, 8, 1, 50)         200       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 8, 1, 50)          0         \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 8, 1, 100)         50100     \n",
      "                                                                 \n",
      " max_pooling2d_13 (MaxPoolin  (None, 3, 1, 100)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, 3, 1, 100)        400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 3, 1, 100)         0         \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 3, 1, 200)         200200    \n",
      "                                                                 \n",
      " max_pooling2d_14 (MaxPoolin  (None, 1, 1, 200)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_14 (Bat  (None, 1, 1, 200)        800       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 1, 1, 200)         0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 200)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 4)                 804       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 269,929\n",
      "Trainable params: 269,159\n",
      "Non-trainable params: 770\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-17 19:11:41.733644: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_2/dropout_10/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 2s 7ms/step - loss: 1.9799 - accuracy: 0.2801\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.6321 - accuracy: 0.2820\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.4637 - accuracy: 0.3119\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.3971 - accuracy: 0.3375\n",
      "Epoch 5/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.3500 - accuracy: 0.3539\n",
      "Epoch 6/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.3010 - accuracy: 0.3856\n",
      "Epoch 7/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.2613 - accuracy: 0.4082\n",
      "Epoch 8/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.2353 - accuracy: 0.4288\n",
      "Epoch 9/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 1.2186 - accuracy: 0.4374\n",
      "Epoch 10/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.1947 - accuracy: 0.4563\n",
      "Epoch 11/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.1842 - accuracy: 0.4645\n",
      "Epoch 12/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.1533 - accuracy: 0.4917\n",
      "Epoch 13/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.1380 - accuracy: 0.5015\n",
      "Epoch 14/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.1166 - accuracy: 0.5194\n",
      "Epoch 15/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.0980 - accuracy: 0.5327\n",
      "Epoch 16/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.0718 - accuracy: 0.5444\n",
      "Epoch 17/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.0603 - accuracy: 0.5514\n",
      "Epoch 18/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.0310 - accuracy: 0.5697\n",
      "Epoch 19/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.0265 - accuracy: 0.5716\n",
      "Epoch 20/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.9993 - accuracy: 0.5874\n",
      "Epoch 21/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.9933 - accuracy: 0.5877\n",
      "Epoch 22/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.9724 - accuracy: 0.5965\n",
      "Epoch 23/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.9598 - accuracy: 0.6076\n",
      "Epoch 24/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.9274 - accuracy: 0.6248\n",
      "Epoch 25/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.9339 - accuracy: 0.6209\n",
      "Epoch 26/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.9111 - accuracy: 0.6371\n",
      "Epoch 27/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.9088 - accuracy: 0.6337\n",
      "Epoch 28/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.9007 - accuracy: 0.6296\n",
      "Epoch 29/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8874 - accuracy: 0.6440\n",
      "Epoch 30/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8756 - accuracy: 0.6498\n",
      "Epoch 31/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8592 - accuracy: 0.6550\n",
      "Epoch 32/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8596 - accuracy: 0.6492\n",
      "Epoch 33/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.8620 - accuracy: 0.6466\n",
      "Epoch 34/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.8389 - accuracy: 0.6650\n",
      "Epoch 35/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.8431 - accuracy: 0.6702\n",
      "Epoch 36/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8255 - accuracy: 0.6667\n",
      "Epoch 37/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8218 - accuracy: 0.6752\n",
      "Epoch 38/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8061 - accuracy: 0.6783\n",
      "Epoch 39/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8077 - accuracy: 0.6774\n",
      "Epoch 40/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8069 - accuracy: 0.6774\n",
      "Epoch 41/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7982 - accuracy: 0.6829\n",
      "Epoch 42/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7949 - accuracy: 0.6791\n",
      "Epoch 43/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7768 - accuracy: 0.6898\n",
      "Epoch 44/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7788 - accuracy: 0.6823\n",
      "Epoch 45/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7686 - accuracy: 0.6944\n",
      "Epoch 46/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7689 - accuracy: 0.6924\n",
      "Epoch 47/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7643 - accuracy: 0.6999\n",
      "Epoch 48/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7639 - accuracy: 0.6953\n",
      "Epoch 49/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.7686 - accuracy: 0.6947\n",
      "Epoch 50/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7646 - accuracy: 0.6960\n",
      "Epoch 51/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7521 - accuracy: 0.7013\n",
      "Epoch 52/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7472 - accuracy: 0.7038\n",
      "Epoch 53/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7477 - accuracy: 0.7017\n",
      "Epoch 54/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7408 - accuracy: 0.7123\n",
      "Epoch 55/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7389 - accuracy: 0.7085\n",
      "Epoch 56/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7340 - accuracy: 0.7087\n",
      "Epoch 57/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7346 - accuracy: 0.7089\n",
      "Epoch 58/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7263 - accuracy: 0.7122\n",
      "Epoch 59/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7146 - accuracy: 0.7169\n",
      "Epoch 60/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7268 - accuracy: 0.7098\n",
      "Epoch 61/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7134 - accuracy: 0.7176\n",
      "Epoch 62/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.7101 - accuracy: 0.7191\n",
      "Epoch 63/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7308 - accuracy: 0.7134\n",
      "Epoch 64/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7082 - accuracy: 0.7196\n",
      "Epoch 65/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7184 - accuracy: 0.7113\n",
      "Epoch 66/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7003 - accuracy: 0.7259\n",
      "Epoch 67/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6955 - accuracy: 0.7220\n",
      "Epoch 68/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6988 - accuracy: 0.7221\n",
      "Epoch 69/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7000 - accuracy: 0.7230\n",
      "Epoch 70/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.6978 - accuracy: 0.7187\n",
      "Epoch 71/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.6912 - accuracy: 0.7286\n",
      "Epoch 72/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6880 - accuracy: 0.7294\n",
      "Epoch 73/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6951 - accuracy: 0.7300\n",
      "Epoch 74/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6760 - accuracy: 0.7311\n",
      "Epoch 75/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.6982 - accuracy: 0.7248\n",
      "Epoch 76/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6811 - accuracy: 0.7313\n",
      "Epoch 77/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6708 - accuracy: 0.7340\n",
      "Epoch 78/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6745 - accuracy: 0.7335\n",
      "Epoch 79/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6861 - accuracy: 0.7278\n",
      "Epoch 80/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6762 - accuracy: 0.7362\n",
      "Epoch 81/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6650 - accuracy: 0.7398\n",
      "Epoch 82/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6653 - accuracy: 0.7413\n",
      "Epoch 83/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6642 - accuracy: 0.7401\n",
      "Epoch 84/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6758 - accuracy: 0.7379\n",
      "Epoch 85/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6601 - accuracy: 0.7361\n",
      "Epoch 86/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6633 - accuracy: 0.7376\n",
      "Epoch 87/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6650 - accuracy: 0.7364\n",
      "Epoch 88/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.6608 - accuracy: 0.7429\n",
      "Epoch 89/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6571 - accuracy: 0.7422\n",
      "Epoch 90/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6368 - accuracy: 0.7509\n",
      "Epoch 91/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6505 - accuracy: 0.7475\n",
      "Epoch 92/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6531 - accuracy: 0.7443\n",
      "Epoch 93/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6443 - accuracy: 0.7440\n",
      "Epoch 94/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6404 - accuracy: 0.7486\n",
      "Epoch 95/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6441 - accuracy: 0.7461\n",
      "Epoch 96/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6460 - accuracy: 0.7459\n",
      "Epoch 97/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6467 - accuracy: 0.7459\n",
      "Epoch 98/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6488 - accuracy: 0.7420\n",
      "Epoch 99/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6379 - accuracy: 0.7500\n",
      "Epoch 100/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6293 - accuracy: 0.7522\n",
      "Test accuracy of the basic CNN model: 0.6839728951454163\n",
      "Shape of X after trimming: (2115, 22, 450)\n",
      "Shape of X after maxpooling: (2115, 22, 225)\n",
      "Shape of X after averaging+noise and concatenating: (4230, 22, 225)\n",
      "Shape of X after subsampling and concatenating: (8460, 22, 225)\n",
      "Shape of X after trimming: (443, 22, 450)\n",
      "Shape of X after maxpooling: (443, 22, 225)\n",
      "Shape of X after averaging+noise and concatenating: (886, 22, 225)\n",
      "Shape of X after subsampling and concatenating: (1772, 22, 225)\n",
      "shape of train sequence (8460, 225, 1, 22)\n",
      "shape of train sequence label (8460, 4)\n",
      "shape of test sequence (1772, 225, 1, 22)\n",
      "shape of test sequence label (1772, 4)\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_15 (Conv2D)          (None, 225, 1, 10)        2210      \n",
      "                                                                 \n",
      " max_pooling2d_15 (MaxPoolin  (None, 75, 1, 10)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_15 (Bat  (None, 75, 1, 10)        40        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 75, 1, 10)         0         \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 75, 1, 25)         2525      \n",
      "                                                                 \n",
      " max_pooling2d_16 (MaxPoolin  (None, 25, 1, 25)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_16 (Bat  (None, 25, 1, 25)        100       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 25, 1, 25)         0         \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 25, 1, 50)         12550     \n",
      "                                                                 \n",
      " max_pooling2d_17 (MaxPoolin  (None, 9, 1, 50)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_17 (Bat  (None, 9, 1, 50)         200       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 9, 1, 50)          0         \n",
      "                                                                 \n",
      " conv2d_18 (Conv2D)          (None, 9, 1, 100)         50100     \n",
      "                                                                 \n",
      " max_pooling2d_18 (MaxPoolin  (None, 3, 1, 100)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_18 (Bat  (None, 3, 1, 100)        400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, 3, 1, 100)         0         \n",
      "                                                                 \n",
      " conv2d_19 (Conv2D)          (None, 3, 1, 200)         200200    \n",
      "                                                                 \n",
      " max_pooling2d_19 (MaxPoolin  (None, 1, 1, 200)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_19 (Bat  (None, 1, 1, 200)        800       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (None, 1, 1, 200)         0         \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 200)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 4)                 804       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 269,929\n",
      "Trainable params: 269,159\n",
      "Non-trainable params: 770\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-17 19:14:08.131337: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_3/dropout_15/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 2s 7ms/step - loss: 2.0314 - accuracy: 0.2668\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.6232 - accuracy: 0.2907\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.4671 - accuracy: 0.3167\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.4010 - accuracy: 0.3304\n",
      "Epoch 5/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 1.3416 - accuracy: 0.3574\n",
      "Epoch 6/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 1.2871 - accuracy: 0.3926\n",
      "Epoch 7/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 1.2494 - accuracy: 0.4196\n",
      "Epoch 8/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.2167 - accuracy: 0.4382\n",
      "Epoch 9/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.1947 - accuracy: 0.4535\n",
      "Epoch 10/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.1808 - accuracy: 0.4645\n",
      "Epoch 11/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.1607 - accuracy: 0.4798\n",
      "Epoch 12/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.1344 - accuracy: 0.5059\n",
      "Epoch 13/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.1212 - accuracy: 0.5157\n",
      "Epoch 14/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.0882 - accuracy: 0.5345\n",
      "Epoch 15/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.0736 - accuracy: 0.5389\n",
      "Epoch 16/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.0560 - accuracy: 0.5544\n",
      "Epoch 17/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 1.0355 - accuracy: 0.5636\n",
      "Epoch 18/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 1.0131 - accuracy: 0.5840\n",
      "Epoch 19/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.9941 - accuracy: 0.5914\n",
      "Epoch 20/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.9767 - accuracy: 0.6004\n",
      "Epoch 21/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.9593 - accuracy: 0.6091\n",
      "Epoch 22/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.9348 - accuracy: 0.6193\n",
      "Epoch 23/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.9244 - accuracy: 0.6228\n",
      "Epoch 24/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.9081 - accuracy: 0.6307\n",
      "Epoch 25/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8901 - accuracy: 0.6434\n",
      "Epoch 26/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8822 - accuracy: 0.6428\n",
      "Epoch 27/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8617 - accuracy: 0.6535\n",
      "Epoch 28/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8519 - accuracy: 0.6576\n",
      "Epoch 29/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8440 - accuracy: 0.6591\n",
      "Epoch 30/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.8289 - accuracy: 0.6708\n",
      "Epoch 31/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.8302 - accuracy: 0.6691\n",
      "Epoch 32/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8292 - accuracy: 0.6746\n",
      "Epoch 33/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8290 - accuracy: 0.6721\n",
      "Epoch 34/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8095 - accuracy: 0.6777\n",
      "Epoch 35/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.8058 - accuracy: 0.6771\n",
      "Epoch 36/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7967 - accuracy: 0.6849\n",
      "Epoch 37/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7888 - accuracy: 0.6862\n",
      "Epoch 38/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7751 - accuracy: 0.6960\n",
      "Epoch 39/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7760 - accuracy: 0.6914\n",
      "Epoch 40/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7698 - accuracy: 0.6970\n",
      "Epoch 41/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7540 - accuracy: 0.7028\n",
      "Epoch 42/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.7600 - accuracy: 0.7038\n",
      "Epoch 43/100\n",
      "133/133 [==============================] - 1s 10ms/step - loss: 0.7537 - accuracy: 0.7060\n",
      "Epoch 44/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.7530 - accuracy: 0.7054\n",
      "Epoch 45/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7459 - accuracy: 0.7048\n",
      "Epoch 46/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7477 - accuracy: 0.7017\n",
      "Epoch 47/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7366 - accuracy: 0.7097\n",
      "Epoch 48/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7380 - accuracy: 0.7103\n",
      "Epoch 49/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7263 - accuracy: 0.7203\n",
      "Epoch 50/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7426 - accuracy: 0.7063\n",
      "Epoch 51/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7209 - accuracy: 0.7160\n",
      "Epoch 52/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7110 - accuracy: 0.7197\n",
      "Epoch 53/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7201 - accuracy: 0.7160\n",
      "Epoch 54/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7157 - accuracy: 0.7206\n",
      "Epoch 55/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7025 - accuracy: 0.7240\n",
      "Epoch 56/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.6849 - accuracy: 0.7326\n",
      "Epoch 57/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.7070 - accuracy: 0.7230\n",
      "Epoch 58/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.6949 - accuracy: 0.7267\n",
      "Epoch 59/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6843 - accuracy: 0.7348\n",
      "Epoch 60/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6848 - accuracy: 0.7311\n",
      "Epoch 61/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6917 - accuracy: 0.7293\n",
      "Epoch 62/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6868 - accuracy: 0.7284\n",
      "Epoch 63/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6842 - accuracy: 0.7356\n",
      "Epoch 64/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6840 - accuracy: 0.7312\n",
      "Epoch 65/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6711 - accuracy: 0.7377\n",
      "Epoch 66/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6762 - accuracy: 0.7326\n",
      "Epoch 67/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6621 - accuracy: 0.7436\n",
      "Epoch 68/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6643 - accuracy: 0.7403\n",
      "Epoch 69/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.6650 - accuracy: 0.7465\n",
      "Epoch 70/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6656 - accuracy: 0.7447\n",
      "Epoch 71/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6646 - accuracy: 0.7436\n",
      "Epoch 72/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6492 - accuracy: 0.7441\n",
      "Epoch 73/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6603 - accuracy: 0.7421\n",
      "Epoch 74/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6562 - accuracy: 0.7460\n",
      "Epoch 75/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6616 - accuracy: 0.7400\n",
      "Epoch 76/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6336 - accuracy: 0.7514\n",
      "Epoch 77/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6437 - accuracy: 0.7461\n",
      "Epoch 78/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6557 - accuracy: 0.7478\n",
      "Epoch 79/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.6460 - accuracy: 0.7478\n",
      "Epoch 80/100\n",
      "133/133 [==============================] - 1s 8ms/step - loss: 0.6368 - accuracy: 0.7566\n",
      "Epoch 81/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.6364 - accuracy: 0.7517\n",
      "Epoch 82/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.6277 - accuracy: 0.7533\n",
      "Epoch 83/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6218 - accuracy: 0.7615\n",
      "Epoch 84/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6307 - accuracy: 0.7570\n",
      "Epoch 85/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6402 - accuracy: 0.7504\n",
      "Epoch 86/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6248 - accuracy: 0.7572\n",
      "Epoch 87/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6098 - accuracy: 0.7613\n",
      "Epoch 88/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6374 - accuracy: 0.7485\n",
      "Epoch 89/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6251 - accuracy: 0.7499\n",
      "Epoch 90/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6204 - accuracy: 0.7574\n",
      "Epoch 91/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6294 - accuracy: 0.7583\n",
      "Epoch 92/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6155 - accuracy: 0.7560\n",
      "Epoch 93/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6161 - accuracy: 0.7630\n",
      "Epoch 94/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.6168 - accuracy: 0.7671\n",
      "Epoch 95/100\n",
      "133/133 [==============================] - 1s 7ms/step - loss: 0.6189 - accuracy: 0.7589\n",
      "Epoch 96/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6062 - accuracy: 0.7678\n",
      "Epoch 97/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6185 - accuracy: 0.7624\n",
      "Epoch 98/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6144 - accuracy: 0.7616\n",
      "Epoch 99/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6011 - accuracy: 0.7675\n",
      "Epoch 100/100\n",
      "133/133 [==============================] - 1s 6ms/step - loss: 0.6069 - accuracy: 0.7652\n",
      "Test accuracy of the basic CNN model: 0.7093679308891296\n"
     ]
    }
   ],
   "source": [
    "time_sequence1=[300,350,400,450]\n",
    "new_CNN_time_sequence=[]\n",
    "for trim_time in time_sequence1:\n",
    "    X_train_sequence,y_train_sequence=process_data(X_train_valid,y_train_valid,trim_time=trim_time)\n",
    "    X_test_sequence,y_test_sequence=process_data(X_test,y_test_original,trim_time=trim_time)\n",
    "    print('shape of train sequence',np.shape(X_train_sequence))\n",
    "    print('shape of train sequence label',np.shape(y_train_sequence))\n",
    "    print('shape of test sequence',np.shape(X_test_sequence))\n",
    "    print('shape of test sequence label',np.shape(y_test_sequence))\n",
    "    learning_rate = 1e-3\n",
    "    epochs=100\n",
    "    cnn_optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    basic_cnn_model=Basic_CNN(trim_time=trim_time)\n",
    "    basic_cnn_model.compile(loss='categorical_crossentropy',\n",
    "                 optimizer=cnn_optimizer,\n",
    "                 metrics=['accuracy'])\n",
    "    cnn_model_results = basic_cnn_model.fit(X_train_sequence,y_train_sequence,\n",
    "             batch_size=64,\n",
    "             epochs=epochs,verbose=True)\n",
    "    cnn_score= basic_cnn_model.evaluate(X_test_sequence,y_test_sequence,verbose=0)\n",
    "    new_CNN_time_sequence.append(cnn_score[1])\n",
    "\n",
    "    print('Test accuracy of the basic CNN model:',cnn_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-17T19:20:00.130880Z",
     "iopub.status.busy": "2023-03-17T19:20:00.130464Z",
     "iopub.status.idle": "2023-03-17T19:20:00.138253Z",
     "shell.execute_reply": "2023-03-17T19:20:00.137120Z",
     "shell.execute_reply.started": "2023-03-17T19:20:00.130846Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6997742652893066,\n",
       " 0.7054176330566406,\n",
       " 0.6839728951454163,\n",
       " 0.7093679308891296]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_CNN_time_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) CNN_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-17T19:25:17.709028Z",
     "iopub.status.busy": "2023-03-17T19:25:17.708108Z",
     "iopub.status.idle": "2023-03-17T19:39:20.640012Z",
     "shell.execute_reply": "2023-03-17T19:39:20.638855Z",
     "shell.execute_reply.started": "2023-03-17T19:25:17.708976Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X after trimming: (2115, 22, 300)\n",
      "Shape of X after maxpooling: (2115, 22, 150)\n",
      "Shape of X after averaging+noise and concatenating: (4230, 22, 150)\n",
      "Shape of X after subsampling and concatenating: (8460, 22, 150)\n",
      "Shape of X after trimming: (443, 22, 300)\n",
      "Shape of X after maxpooling: (443, 22, 150)\n",
      "Shape of X after averaging+noise and concatenating: (886, 22, 150)\n",
      "Shape of X after subsampling and concatenating: (1772, 22, 150)\n",
      "shape of train sequence (8460, 150, 22)\n",
      "shape of train sequence label (8460, 4)\n",
      "shape of test sequence (1772, 150, 22)\n",
      "shape of test sequence label (1772, 4)\n",
      "Epoch 1/100\n",
      "133/133 [==============================] - 8s 13ms/step - loss: 1.3790 - accuracy: 0.2876\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 1.3395 - accuracy: 0.3500\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 1.2750 - accuracy: 0.4057\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 1.2282 - accuracy: 0.4364\n",
      "Epoch 5/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.1773 - accuracy: 0.4703\n",
      "Epoch 6/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 1.1433 - accuracy: 0.5058\n",
      "Epoch 7/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 1.1160 - accuracy: 0.5230\n",
      "Epoch 8/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 1.0900 - accuracy: 0.5429\n",
      "Epoch 9/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 1.0478 - accuracy: 0.5595\n",
      "Epoch 10/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 1.0226 - accuracy: 0.5752\n",
      "Epoch 11/100\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.9893 - accuracy: 0.5961\n",
      "Epoch 12/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.9697 - accuracy: 0.6074\n",
      "Epoch 13/100\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.9228 - accuracy: 0.6277\n",
      "Epoch 14/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.9085 - accuracy: 0.6323\n",
      "Epoch 15/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.8953 - accuracy: 0.6414\n",
      "Epoch 16/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.8765 - accuracy: 0.6528\n",
      "Epoch 17/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.8720 - accuracy: 0.6465\n",
      "Epoch 18/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.8329 - accuracy: 0.6706\n",
      "Epoch 19/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.8324 - accuracy: 0.6740\n",
      "Epoch 20/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.8005 - accuracy: 0.6842\n",
      "Epoch 21/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.7898 - accuracy: 0.6937\n",
      "Epoch 22/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.7813 - accuracy: 0.6973\n",
      "Epoch 23/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.7724 - accuracy: 0.6975\n",
      "Epoch 24/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.7672 - accuracy: 0.6972\n",
      "Epoch 25/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7709 - accuracy: 0.6987\n",
      "Epoch 26/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.7407 - accuracy: 0.7095\n",
      "Epoch 27/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.7430 - accuracy: 0.7067\n",
      "Epoch 28/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.7428 - accuracy: 0.7110\n",
      "Epoch 29/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.7232 - accuracy: 0.7222\n",
      "Epoch 30/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.7104 - accuracy: 0.7268\n",
      "Epoch 31/100\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.6944 - accuracy: 0.7372\n",
      "Epoch 32/100\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.6999 - accuracy: 0.7309\n",
      "Epoch 33/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.6905 - accuracy: 0.7383\n",
      "Epoch 34/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.6887 - accuracy: 0.7342\n",
      "Epoch 35/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.6802 - accuracy: 0.7371\n",
      "Epoch 36/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.6653 - accuracy: 0.7455\n",
      "Epoch 37/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.6629 - accuracy: 0.7413\n",
      "Epoch 38/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.6592 - accuracy: 0.7470\n",
      "Epoch 39/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.6596 - accuracy: 0.7437\n",
      "Epoch 40/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.6556 - accuracy: 0.7480\n",
      "Epoch 41/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6396 - accuracy: 0.7597\n",
      "Epoch 42/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6462 - accuracy: 0.7535\n",
      "Epoch 43/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.6339 - accuracy: 0.7537\n",
      "Epoch 44/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.6342 - accuracy: 0.7589\n",
      "Epoch 45/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.6259 - accuracy: 0.7622\n",
      "Epoch 46/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6181 - accuracy: 0.7669\n",
      "Epoch 47/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.6170 - accuracy: 0.7715\n",
      "Epoch 48/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6158 - accuracy: 0.7650\n",
      "Epoch 49/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6085 - accuracy: 0.7664\n",
      "Epoch 50/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.6178 - accuracy: 0.7691\n",
      "Epoch 51/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.5896 - accuracy: 0.7743\n",
      "Epoch 52/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.5957 - accuracy: 0.7751\n",
      "Epoch 53/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.5987 - accuracy: 0.7742\n",
      "Epoch 54/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5893 - accuracy: 0.7764\n",
      "Epoch 55/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.5977 - accuracy: 0.7753\n",
      "Epoch 56/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.5770 - accuracy: 0.7869\n",
      "Epoch 57/100\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.5710 - accuracy: 0.7826\n",
      "Epoch 58/100\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.5805 - accuracy: 0.7793\n",
      "Epoch 59/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5758 - accuracy: 0.7827\n",
      "Epoch 60/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.5748 - accuracy: 0.7832\n",
      "Epoch 61/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5541 - accuracy: 0.7948\n",
      "Epoch 62/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5721 - accuracy: 0.7819\n",
      "Epoch 63/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.5672 - accuracy: 0.7855\n",
      "Epoch 64/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5603 - accuracy: 0.7894\n",
      "Epoch 65/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.5741 - accuracy: 0.7830\n",
      "Epoch 66/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5496 - accuracy: 0.7961\n",
      "Epoch 67/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5557 - accuracy: 0.7879\n",
      "Epoch 68/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5354 - accuracy: 0.7980\n",
      "Epoch 69/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.5343 - accuracy: 0.7987\n",
      "Epoch 70/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5616 - accuracy: 0.7913\n",
      "Epoch 71/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5375 - accuracy: 0.7950\n",
      "Epoch 72/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.5387 - accuracy: 0.8041\n",
      "Epoch 73/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5283 - accuracy: 0.8024\n",
      "Epoch 74/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5464 - accuracy: 0.7950\n",
      "Epoch 75/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.5205 - accuracy: 0.7991\n",
      "Epoch 76/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.5305 - accuracy: 0.8005\n",
      "Epoch 77/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.5235 - accuracy: 0.8060\n",
      "Epoch 78/100\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.5074 - accuracy: 0.8077\n",
      "Epoch 79/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5199 - accuracy: 0.8000\n",
      "Epoch 80/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5083 - accuracy: 0.8104\n",
      "Epoch 81/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5204 - accuracy: 0.8044\n",
      "Epoch 82/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5166 - accuracy: 0.8037\n",
      "Epoch 83/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.5040 - accuracy: 0.8121\n",
      "Epoch 84/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5089 - accuracy: 0.8077\n",
      "Epoch 85/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5096 - accuracy: 0.8043\n",
      "Epoch 86/100\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.5019 - accuracy: 0.8106\n",
      "Epoch 87/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5149 - accuracy: 0.8091\n",
      "Epoch 88/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4810 - accuracy: 0.8204\n",
      "Epoch 89/100\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.5059 - accuracy: 0.8117\n",
      "Epoch 90/100\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.5172 - accuracy: 0.8040\n",
      "Epoch 91/100\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.4953 - accuracy: 0.8144\n",
      "Epoch 92/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.4970 - accuracy: 0.8165\n",
      "Epoch 93/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.4945 - accuracy: 0.8124\n",
      "Epoch 94/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4828 - accuracy: 0.8186\n",
      "Epoch 95/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5012 - accuracy: 0.8128\n",
      "Epoch 96/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4853 - accuracy: 0.8164\n",
      "Epoch 97/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4975 - accuracy: 0.8152\n",
      "Epoch 98/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4844 - accuracy: 0.8171\n",
      "Epoch 99/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4889 - accuracy: 0.8177\n",
      "Epoch 100/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4812 - accuracy: 0.8206\n",
      "Test accuracy of the CNN_LSTM model: 0.7014672756195068\n",
      "Shape of X after trimming: (2115, 22, 350)\n",
      "Shape of X after maxpooling: (2115, 22, 175)\n",
      "Shape of X after averaging+noise and concatenating: (4230, 22, 175)\n",
      "Shape of X after subsampling and concatenating: (8460, 22, 175)\n",
      "Shape of X after trimming: (443, 22, 350)\n",
      "Shape of X after maxpooling: (443, 22, 175)\n",
      "Shape of X after averaging+noise and concatenating: (886, 22, 175)\n",
      "Shape of X after subsampling and concatenating: (1772, 22, 175)\n",
      "shape of train sequence (8460, 175, 22)\n",
      "shape of train sequence label (8460, 4)\n",
      "shape of test sequence (1772, 175, 22)\n",
      "shape of test sequence label (1772, 4)\n",
      "Epoch 1/100\n",
      "133/133 [==============================] - 6s 13ms/step - loss: 1.3713 - accuracy: 0.2965\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 1.3183 - accuracy: 0.3684\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 1.2531 - accuracy: 0.4191\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 1.2002 - accuracy: 0.4622\n",
      "Epoch 5/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 1.1476 - accuracy: 0.4989\n",
      "Epoch 6/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0882 - accuracy: 0.5366\n",
      "Epoch 7/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 1.0351 - accuracy: 0.5704\n",
      "Epoch 8/100\n",
      "133/133 [==============================] - 1s 11ms/step - loss: 0.9995 - accuracy: 0.5941\n",
      "Epoch 9/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.9698 - accuracy: 0.6050\n",
      "Epoch 10/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.9446 - accuracy: 0.6173\n",
      "Epoch 11/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.9151 - accuracy: 0.6366\n",
      "Epoch 12/100\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.8891 - accuracy: 0.6413\n",
      "Epoch 13/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.8671 - accuracy: 0.6508\n",
      "Epoch 14/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.8493 - accuracy: 0.6586\n",
      "Epoch 15/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.8335 - accuracy: 0.6714\n",
      "Epoch 16/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.8223 - accuracy: 0.6758\n",
      "Epoch 17/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.8041 - accuracy: 0.6852\n",
      "Epoch 18/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.7761 - accuracy: 0.6931\n",
      "Epoch 19/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.7870 - accuracy: 0.6930\n",
      "Epoch 20/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.7785 - accuracy: 0.6936\n",
      "Epoch 21/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.7529 - accuracy: 0.7028\n",
      "Epoch 22/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.7421 - accuracy: 0.7111\n",
      "Epoch 23/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.7317 - accuracy: 0.7116\n",
      "Epoch 24/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.7165 - accuracy: 0.7226\n",
      "Epoch 25/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.7162 - accuracy: 0.7195\n",
      "Epoch 26/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.6848 - accuracy: 0.7376\n",
      "Epoch 27/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6826 - accuracy: 0.7346\n",
      "Epoch 28/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6852 - accuracy: 0.7378\n",
      "Epoch 29/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6742 - accuracy: 0.7383\n",
      "Epoch 30/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6773 - accuracy: 0.7402\n",
      "Epoch 31/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.6570 - accuracy: 0.7460\n",
      "Epoch 32/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6563 - accuracy: 0.7439\n",
      "Epoch 33/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.6599 - accuracy: 0.7434\n",
      "Epoch 34/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6316 - accuracy: 0.7579\n",
      "Epoch 35/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6357 - accuracy: 0.7578\n",
      "Epoch 36/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6266 - accuracy: 0.7619\n",
      "Epoch 37/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6158 - accuracy: 0.7655\n",
      "Epoch 38/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6083 - accuracy: 0.7667\n",
      "Epoch 39/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6046 - accuracy: 0.7710\n",
      "Epoch 40/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.6003 - accuracy: 0.7657\n",
      "Epoch 41/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5969 - accuracy: 0.7706\n",
      "Epoch 42/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5995 - accuracy: 0.7695\n",
      "Epoch 43/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5920 - accuracy: 0.7704\n",
      "Epoch 44/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5980 - accuracy: 0.7732\n",
      "Epoch 45/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5811 - accuracy: 0.7787\n",
      "Epoch 46/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5914 - accuracy: 0.7748\n",
      "Epoch 47/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5827 - accuracy: 0.7770\n",
      "Epoch 48/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5523 - accuracy: 0.7888\n",
      "Epoch 49/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5699 - accuracy: 0.7823\n",
      "Epoch 50/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5568 - accuracy: 0.7909\n",
      "Epoch 51/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5590 - accuracy: 0.7852\n",
      "Epoch 52/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5402 - accuracy: 0.7950\n",
      "Epoch 53/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5478 - accuracy: 0.7911\n",
      "Epoch 54/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5385 - accuracy: 0.7996\n",
      "Epoch 55/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5375 - accuracy: 0.7975\n",
      "Epoch 56/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5338 - accuracy: 0.7976\n",
      "Epoch 57/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5288 - accuracy: 0.8017\n",
      "Epoch 58/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5195 - accuracy: 0.8027\n",
      "Epoch 59/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5182 - accuracy: 0.8040\n",
      "Epoch 60/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5395 - accuracy: 0.7900\n",
      "Epoch 61/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5186 - accuracy: 0.8066\n",
      "Epoch 62/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5202 - accuracy: 0.8017\n",
      "Epoch 63/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5194 - accuracy: 0.8025\n",
      "Epoch 64/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.5098 - accuracy: 0.8052\n",
      "Epoch 65/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5268 - accuracy: 0.7993\n",
      "Epoch 66/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5096 - accuracy: 0.8090\n",
      "Epoch 67/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4943 - accuracy: 0.8089\n",
      "Epoch 68/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5026 - accuracy: 0.8119\n",
      "Epoch 69/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5005 - accuracy: 0.8092\n",
      "Epoch 70/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5078 - accuracy: 0.8099\n",
      "Epoch 71/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4932 - accuracy: 0.8106\n",
      "Epoch 72/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4842 - accuracy: 0.8194\n",
      "Epoch 73/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4769 - accuracy: 0.8208\n",
      "Epoch 74/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4779 - accuracy: 0.8193\n",
      "Epoch 75/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4916 - accuracy: 0.8165\n",
      "Epoch 76/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4721 - accuracy: 0.8234\n",
      "Epoch 77/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4761 - accuracy: 0.8212\n",
      "Epoch 78/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4726 - accuracy: 0.8246\n",
      "Epoch 79/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4821 - accuracy: 0.8155\n",
      "Epoch 80/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4742 - accuracy: 0.8260\n",
      "Epoch 81/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4761 - accuracy: 0.8234\n",
      "Epoch 82/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.4573 - accuracy: 0.8279\n",
      "Epoch 83/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.4638 - accuracy: 0.8288\n",
      "Epoch 84/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4693 - accuracy: 0.8222\n",
      "Epoch 85/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4459 - accuracy: 0.8307\n",
      "Epoch 86/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4777 - accuracy: 0.8177\n",
      "Epoch 87/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4665 - accuracy: 0.8264\n",
      "Epoch 88/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4615 - accuracy: 0.8249\n",
      "Epoch 89/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4663 - accuracy: 0.8240\n",
      "Epoch 90/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4506 - accuracy: 0.8273\n",
      "Epoch 91/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4587 - accuracy: 0.8287\n",
      "Epoch 92/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.4732 - accuracy: 0.8260\n",
      "Epoch 93/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4458 - accuracy: 0.8317\n",
      "Epoch 94/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4508 - accuracy: 0.8317\n",
      "Epoch 95/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4477 - accuracy: 0.8304\n",
      "Epoch 96/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4471 - accuracy: 0.8319\n",
      "Epoch 97/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.4478 - accuracy: 0.8331\n",
      "Epoch 98/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4379 - accuracy: 0.8325\n",
      "Epoch 99/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4457 - accuracy: 0.8374\n",
      "Epoch 100/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4295 - accuracy: 0.8400\n",
      "Test accuracy of the CNN_LSTM model: 0.7240406274795532\n",
      "Shape of X after trimming: (2115, 22, 400)\n",
      "Shape of X after maxpooling: (2115, 22, 200)\n",
      "Shape of X after averaging+noise and concatenating: (4230, 22, 200)\n",
      "Shape of X after subsampling and concatenating: (8460, 22, 200)\n",
      "Shape of X after trimming: (443, 22, 400)\n",
      "Shape of X after maxpooling: (443, 22, 200)\n",
      "Shape of X after averaging+noise and concatenating: (886, 22, 200)\n",
      "Shape of X after subsampling and concatenating: (1772, 22, 200)\n",
      "shape of train sequence (8460, 200, 22)\n",
      "shape of train sequence label (8460, 4)\n",
      "shape of test sequence (1772, 200, 22)\n",
      "shape of test sequence label (1772, 4)\n",
      "Epoch 1/100\n",
      "133/133 [==============================] - 7s 13ms/step - loss: 1.3747 - accuracy: 0.2970\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 1.3082 - accuracy: 0.3783\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 1.2368 - accuracy: 0.4376\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 1.1685 - accuracy: 0.4855\n",
      "Epoch 5/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 1.1038 - accuracy: 0.5290\n",
      "Epoch 6/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 1.0591 - accuracy: 0.5537\n",
      "Epoch 7/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 1.0116 - accuracy: 0.5749\n",
      "Epoch 8/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.9699 - accuracy: 0.6025\n",
      "Epoch 9/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.9396 - accuracy: 0.6246\n",
      "Epoch 10/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.9048 - accuracy: 0.6346\n",
      "Epoch 11/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.8824 - accuracy: 0.6480\n",
      "Epoch 12/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.8650 - accuracy: 0.6596\n",
      "Epoch 13/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.8420 - accuracy: 0.6680\n",
      "Epoch 14/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.8281 - accuracy: 0.6728\n",
      "Epoch 15/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.8207 - accuracy: 0.6754\n",
      "Epoch 16/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.8045 - accuracy: 0.6850\n",
      "Epoch 17/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.7824 - accuracy: 0.6923\n",
      "Epoch 18/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.7649 - accuracy: 0.7028\n",
      "Epoch 19/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.7554 - accuracy: 0.7039\n",
      "Epoch 20/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.7531 - accuracy: 0.7051\n",
      "Epoch 21/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.7313 - accuracy: 0.7154\n",
      "Epoch 22/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.7096 - accuracy: 0.7260\n",
      "Epoch 23/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.7307 - accuracy: 0.7151\n",
      "Epoch 24/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7028 - accuracy: 0.7271\n",
      "Epoch 25/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.6985 - accuracy: 0.7264\n",
      "Epoch 26/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6682 - accuracy: 0.7391\n",
      "Epoch 27/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6504 - accuracy: 0.7547\n",
      "Epoch 28/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6725 - accuracy: 0.7439\n",
      "Epoch 29/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6656 - accuracy: 0.7450\n",
      "Epoch 30/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6384 - accuracy: 0.7558\n",
      "Epoch 31/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.6411 - accuracy: 0.7515\n",
      "Epoch 32/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6262 - accuracy: 0.7556\n",
      "Epoch 33/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6300 - accuracy: 0.7595\n",
      "Epoch 34/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6236 - accuracy: 0.7643\n",
      "Epoch 35/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.6223 - accuracy: 0.7595\n",
      "Epoch 36/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6227 - accuracy: 0.7598\n",
      "Epoch 37/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6078 - accuracy: 0.7686\n",
      "Epoch 38/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5970 - accuracy: 0.7735\n",
      "Epoch 39/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5825 - accuracy: 0.7837\n",
      "Epoch 40/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5825 - accuracy: 0.7813\n",
      "Epoch 41/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5715 - accuracy: 0.7825\n",
      "Epoch 42/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5726 - accuracy: 0.7845\n",
      "Epoch 43/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5657 - accuracy: 0.7851\n",
      "Epoch 44/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5681 - accuracy: 0.7864\n",
      "Epoch 45/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5578 - accuracy: 0.7850\n",
      "Epoch 46/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5514 - accuracy: 0.7987\n",
      "Epoch 47/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5600 - accuracy: 0.7878\n",
      "Epoch 48/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5445 - accuracy: 0.7981\n",
      "Epoch 49/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5442 - accuracy: 0.7955\n",
      "Epoch 50/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5326 - accuracy: 0.8014\n",
      "Epoch 51/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5290 - accuracy: 0.7985\n",
      "Epoch 52/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5232 - accuracy: 0.8020\n",
      "Epoch 53/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5281 - accuracy: 0.7993\n",
      "Epoch 54/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.5162 - accuracy: 0.8063\n",
      "Epoch 55/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5232 - accuracy: 0.8015\n",
      "Epoch 56/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5176 - accuracy: 0.8060\n",
      "Epoch 57/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5209 - accuracy: 0.8047\n",
      "Epoch 58/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5009 - accuracy: 0.8106\n",
      "Epoch 59/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5026 - accuracy: 0.8118\n",
      "Epoch 60/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5017 - accuracy: 0.8108\n",
      "Epoch 61/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5064 - accuracy: 0.8110\n",
      "Epoch 62/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5046 - accuracy: 0.8126\n",
      "Epoch 63/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5033 - accuracy: 0.8158\n",
      "Epoch 64/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4850 - accuracy: 0.8194\n",
      "Epoch 65/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4997 - accuracy: 0.8076\n",
      "Epoch 66/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5144 - accuracy: 0.8066\n",
      "Epoch 67/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4886 - accuracy: 0.8162\n",
      "Epoch 68/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4857 - accuracy: 0.8214\n",
      "Epoch 69/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4719 - accuracy: 0.8214\n",
      "Epoch 70/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4659 - accuracy: 0.8291\n",
      "Epoch 71/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4680 - accuracy: 0.8254\n",
      "Epoch 72/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4602 - accuracy: 0.8246\n",
      "Epoch 73/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4764 - accuracy: 0.8239\n",
      "Epoch 74/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4784 - accuracy: 0.8221\n",
      "Epoch 75/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4470 - accuracy: 0.8320\n",
      "Epoch 76/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4651 - accuracy: 0.8222\n",
      "Epoch 77/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4548 - accuracy: 0.8266\n",
      "Epoch 78/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4688 - accuracy: 0.8234\n",
      "Epoch 79/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4497 - accuracy: 0.8337\n",
      "Epoch 80/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4353 - accuracy: 0.8375\n",
      "Epoch 81/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4362 - accuracy: 0.8384\n",
      "Epoch 82/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4580 - accuracy: 0.8299\n",
      "Epoch 83/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4405 - accuracy: 0.8346\n",
      "Epoch 84/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4518 - accuracy: 0.8324\n",
      "Epoch 85/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4393 - accuracy: 0.8374\n",
      "Epoch 86/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4435 - accuracy: 0.8375\n",
      "Epoch 87/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4321 - accuracy: 0.8414\n",
      "Epoch 88/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4250 - accuracy: 0.8414\n",
      "Epoch 89/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4490 - accuracy: 0.8365\n",
      "Epoch 90/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4390 - accuracy: 0.8420\n",
      "Epoch 91/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4295 - accuracy: 0.8378\n",
      "Epoch 92/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4306 - accuracy: 0.8384\n",
      "Epoch 93/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4238 - accuracy: 0.8415\n",
      "Epoch 94/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4392 - accuracy: 0.8377\n",
      "Epoch 95/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4376 - accuracy: 0.8371\n",
      "Epoch 96/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4423 - accuracy: 0.8343\n",
      "Epoch 97/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4169 - accuracy: 0.8468\n",
      "Epoch 98/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4029 - accuracy: 0.8514\n",
      "Epoch 99/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4244 - accuracy: 0.8423\n",
      "Epoch 100/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4121 - accuracy: 0.8468\n",
      "Test accuracy of the CNN_LSTM model: 0.7240406274795532\n",
      "Shape of X after trimming: (2115, 22, 450)\n",
      "Shape of X after maxpooling: (2115, 22, 225)\n",
      "Shape of X after averaging+noise and concatenating: (4230, 22, 225)\n",
      "Shape of X after subsampling and concatenating: (8460, 22, 225)\n",
      "Shape of X after trimming: (443, 22, 450)\n",
      "Shape of X after maxpooling: (443, 22, 225)\n",
      "Shape of X after averaging+noise and concatenating: (886, 22, 225)\n",
      "Shape of X after subsampling and concatenating: (1772, 22, 225)\n",
      "shape of train sequence (8460, 225, 22)\n",
      "shape of train sequence label (8460, 4)\n",
      "shape of test sequence (1772, 225, 22)\n",
      "shape of test sequence label (1772, 4)\n",
      "Epoch 1/100\n",
      "133/133 [==============================] - 6s 14ms/step - loss: 1.3678 - accuracy: 0.3021\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 1.2938 - accuracy: 0.3895\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 1.2273 - accuracy: 0.4379\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 1.1834 - accuracy: 0.4700\n",
      "Epoch 5/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 1.1489 - accuracy: 0.5040\n",
      "Epoch 6/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 1.0926 - accuracy: 0.5388\n",
      "Epoch 7/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 1.0595 - accuracy: 0.5554\n",
      "Epoch 8/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 1.0204 - accuracy: 0.5811\n",
      "Epoch 9/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.9671 - accuracy: 0.6131\n",
      "Epoch 10/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.9324 - accuracy: 0.6296\n",
      "Epoch 11/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.9119 - accuracy: 0.6350\n",
      "Epoch 12/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.8986 - accuracy: 0.6404\n",
      "Epoch 13/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.8617 - accuracy: 0.6670\n",
      "Epoch 14/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.8424 - accuracy: 0.6719\n",
      "Epoch 15/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.8265 - accuracy: 0.6809\n",
      "Epoch 16/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8045 - accuracy: 0.6918\n",
      "Epoch 17/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.7897 - accuracy: 0.6871\n",
      "Epoch 18/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.7704 - accuracy: 0.6981\n",
      "Epoch 19/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.7595 - accuracy: 0.7066\n",
      "Epoch 20/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7430 - accuracy: 0.7148\n",
      "Epoch 21/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.7452 - accuracy: 0.7086\n",
      "Epoch 22/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7144 - accuracy: 0.7291\n",
      "Epoch 23/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.7112 - accuracy: 0.7280\n",
      "Epoch 24/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.7076 - accuracy: 0.7283\n",
      "Epoch 25/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6978 - accuracy: 0.7333\n",
      "Epoch 26/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6859 - accuracy: 0.7331\n",
      "Epoch 27/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.6723 - accuracy: 0.7427\n",
      "Epoch 28/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.6520 - accuracy: 0.7560\n",
      "Epoch 29/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.6613 - accuracy: 0.7459\n",
      "Epoch 30/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6490 - accuracy: 0.7507\n",
      "Epoch 31/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6460 - accuracy: 0.7532\n",
      "Epoch 32/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6526 - accuracy: 0.7524\n",
      "Epoch 33/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6228 - accuracy: 0.7661\n",
      "Epoch 34/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6259 - accuracy: 0.7642\n",
      "Epoch 35/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6075 - accuracy: 0.7719\n",
      "Epoch 36/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.6066 - accuracy: 0.7712\n",
      "Epoch 37/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6102 - accuracy: 0.7691\n",
      "Epoch 38/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5891 - accuracy: 0.7816\n",
      "Epoch 39/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5961 - accuracy: 0.7768\n",
      "Epoch 40/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5774 - accuracy: 0.7839\n",
      "Epoch 41/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5773 - accuracy: 0.7929\n",
      "Epoch 42/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5701 - accuracy: 0.7887\n",
      "Epoch 43/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5704 - accuracy: 0.7869\n",
      "Epoch 44/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5637 - accuracy: 0.7913\n",
      "Epoch 45/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5505 - accuracy: 0.7970\n",
      "Epoch 46/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5365 - accuracy: 0.7994\n",
      "Epoch 47/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5530 - accuracy: 0.7942\n",
      "Epoch 48/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5330 - accuracy: 0.7974\n",
      "Epoch 49/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5352 - accuracy: 0.7974\n",
      "Epoch 50/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5480 - accuracy: 0.7894\n",
      "Epoch 51/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5454 - accuracy: 0.8007\n",
      "Epoch 52/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5230 - accuracy: 0.8058\n",
      "Epoch 53/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5184 - accuracy: 0.8092\n",
      "Epoch 54/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5108 - accuracy: 0.8084\n",
      "Epoch 55/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5086 - accuracy: 0.8123\n",
      "Epoch 56/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5036 - accuracy: 0.8122\n",
      "Epoch 57/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5233 - accuracy: 0.8018\n",
      "Epoch 58/100\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.4960 - accuracy: 0.8121\n",
      "Epoch 59/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4908 - accuracy: 0.8160\n",
      "Epoch 60/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4908 - accuracy: 0.8135\n",
      "Epoch 61/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5045 - accuracy: 0.8089\n",
      "Epoch 62/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4901 - accuracy: 0.8147\n",
      "Epoch 63/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4829 - accuracy: 0.8186\n",
      "Epoch 64/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4909 - accuracy: 0.8194\n",
      "Epoch 65/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4719 - accuracy: 0.8201\n",
      "Epoch 66/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4933 - accuracy: 0.8136\n",
      "Epoch 67/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4818 - accuracy: 0.8187\n",
      "Epoch 68/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4681 - accuracy: 0.8232\n",
      "Epoch 69/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4880 - accuracy: 0.8174\n",
      "Epoch 70/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4710 - accuracy: 0.8251\n",
      "Epoch 71/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4602 - accuracy: 0.8278\n",
      "Epoch 72/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4689 - accuracy: 0.8283\n",
      "Epoch 73/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4754 - accuracy: 0.8243\n",
      "Epoch 74/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4620 - accuracy: 0.8316\n",
      "Epoch 75/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4520 - accuracy: 0.8337\n",
      "Epoch 76/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4621 - accuracy: 0.8297\n",
      "Epoch 77/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4487 - accuracy: 0.8329\n",
      "Epoch 78/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4482 - accuracy: 0.8356\n",
      "Epoch 79/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4386 - accuracy: 0.8372\n",
      "Epoch 80/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4247 - accuracy: 0.8466\n",
      "Epoch 81/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4430 - accuracy: 0.8348\n",
      "Epoch 82/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4407 - accuracy: 0.8370\n",
      "Epoch 83/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4315 - accuracy: 0.8390\n",
      "Epoch 84/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4349 - accuracy: 0.8391\n",
      "Epoch 85/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4344 - accuracy: 0.8376\n",
      "Epoch 86/100\n",
      "133/133 [==============================] - 2s 11ms/step - loss: 0.4241 - accuracy: 0.8429\n",
      "Epoch 87/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4408 - accuracy: 0.8344\n",
      "Epoch 88/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4257 - accuracy: 0.8417\n",
      "Epoch 89/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4195 - accuracy: 0.8472\n",
      "Epoch 90/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4289 - accuracy: 0.8370\n",
      "Epoch 91/100\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.4145 - accuracy: 0.8460\n",
      "Epoch 92/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4279 - accuracy: 0.8436\n",
      "Epoch 93/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4189 - accuracy: 0.8421\n",
      "Epoch 94/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4234 - accuracy: 0.8435\n",
      "Epoch 95/100\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.4122 - accuracy: 0.8539\n",
      "Epoch 96/100\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4098 - accuracy: 0.8478\n",
      "Epoch 97/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4101 - accuracy: 0.8483\n",
      "Epoch 98/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4038 - accuracy: 0.8546\n",
      "Epoch 99/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4134 - accuracy: 0.8450\n",
      "Epoch 100/100\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4130 - accuracy: 0.8485\n",
      "Test accuracy of the CNN_LSTM model: 0.703160285949707\n"
     ]
    }
   ],
   "source": [
    "#try new time sequence on CNN_LSTM\n",
    "time_sequence1=[300,350,400,450]# between 250-500\n",
    "CNN_LSTM_new_sequence=[]\n",
    "for trim_time in time_sequence1:\n",
    "    X_train_sequence,y_train_sequence=process_data(X_train_valid,y_train_valid,trim_time=trim_time)\n",
    "    X_test_sequence,y_test_sequence=process_data(X_test,y_test_original,trim_time=trim_time)\n",
    "    X_train_sequence = X_train_sequence.reshape((X_train_sequence.shape[0], X_train_sequence.shape[1], X_train_sequence.shape[3]))\n",
    "    X_test_sequence = X_test_sequence.reshape((X_test_sequence.shape[0], X_test_sequence.shape[1], X_test_sequence.shape[3]))\n",
    "    print('shape of train sequence',np.shape(X_train_sequence))\n",
    "    print('shape of train sequence label',np.shape(y_train_sequence))\n",
    "    print('shape of test sequence',np.shape(X_test_sequence))\n",
    "    print('shape of test sequence label',np.shape(y_test_sequence))\n",
    "    learning_rate = 1e-3\n",
    "    epochs=100\n",
    "    cnn_optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    hybrid_CNN_LSTM_model=CNN_LSTM(trim_time=trim_time)\n",
    "    hybrid_CNN_LSTM_model.compile(loss='categorical_crossentropy',\n",
    "                 optimizer=cnn_optimizer,\n",
    "                 metrics=['accuracy'])\n",
    "    hybrid_CNN_LSTM_results =  hybrid_CNN_LSTM_model.fit(X_train_sequence,y_train_sequence,\n",
    "             batch_size=64,\n",
    "             epochs=epochs,verbose=True)\n",
    "    cnn_lstm_score= hybrid_CNN_LSTM_model.evaluate(X_test_sequence,y_test_sequence,verbose=0)\n",
    "    CNN_LSTM_new_sequence.append(cnn_lstm_score[1])\n",
    "    print('Test accuracy of the CNN_LSTM model:',cnn_lstm_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.status.busy": "2023-03-17T09:09:28.153715Z",
     "iopub.status.idle": "2023-03-17T09:09:28.154221Z",
     "shell.execute_reply": "2023-03-17T09:09:28.153995Z",
     "shell.execute_reply.started": "2023-03-17T09:09:28.153970Z"
    },
    "executionInfo": {
     "elapsed": 240,
     "status": "ok",
     "timestamp": 1678912593037,
     "user": {
      "displayName": "CHUSHU SHEN",
      "userId": "01882361236256093541"
     },
     "user_tz": 420
    },
    "id": "s_8VKInVVOr1",
    "outputId": "ca57eba5-b920-4a10-b514-cc22b1420c85"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
